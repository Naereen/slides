% \documentclass[slideopt,A4,11pt,english,aspectratio=169,]{beamer}
\documentclass[slideopt,A4,11pt,english,]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{environ}
\usepackage{multimedia} % videos in beamer
\usepackage{lmodern}
\usepackage{fourier-orns} % danger
\usepackage{amsmath,amsfonts,amssymb} % Maths.
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{eso-pic}

% --- Figures Tikz
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{shapes.symbols}
\usetikzlibrary{backgrounds}
\usetikzlibrary{chains}
\usepackage{tikzsymbols}  % http://texdoc.net/texmf-dist/doc/latex/comprehensive/symbols-a4.pdf for the \Coffeecup{} command!

\usepackage{layout}
\usepackage{xcolor,colortbl}

\useoutertheme{infolines}

% \usefonttheme{serif} % default family is serif
% % \usepackage{palatino}              % Use the Palatino font % XXX remove if it is ugly ?
% \usepackage{mathpazo}
% \usepackage{tgpagella} % alternative to MinionPro

\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage[absolute,showboxes,overlay]{textpos}
\TPshowboxesfalse
\textblockorigin{0mm}{0mm}

%%%%% COLORS
\xdefinecolor{myorange}{rgb}{1.,0.15,0.}
\newcommand{\blue}{\color{blue}}
\newcommand{\red}{\color{red}}
\newcommand{\orange}{\color{orange}}
\newcommand{\black}{\color{black}}
\newcommand{\gray}{\color{gray}}

\definecolor{lightgray}{RGB}{200,200,200}  % rgb(200,200,200)
\definecolor{blackblue}{RGB}{19,19,59}     % rgb(19,19,59)
\definecolor{bleu}{RGB}{0,0,204}           % rgb(0,0,204)
\definecolor{lightblue}{RGB}{50,50,204}    % rgb(50,50,204)
\definecolor{deeppurple}{RGB}{102,0,204}   % rgb(102,0,204)
\definecolor{darkgreen}{RGB}{0,100,0}      % rgb(0,100,0)
\definecolor{yellowgreen}{RGB}{200,215,0}  % rgb(200,215,0)
\definecolor{bluegreen}{RGB}{0,185,140}    % rgb(0,185,140)
\definecolor{lightgold}{RGB}{255,180,0}    % rgb(255,180,0)
\definecolor{gold}{RGB}{175,100,0}         % rgb(175,100,0)
\definecolor{strongred}{RGB}{255,0,0}      % rgb(255,0,0)
\definecolor{normalred}{RGB}{204,0,0}      % rgb(204,0,0)
\definecolor{darkred}{RGB}{174,0,0}        % rgb(174,0,0)
\definecolor{darkblue}{RGB}{0,0,174}       % rgb(0,0,174)
\definecolor{darkpurple}{RGB}{114,0,114}   % rgb(114,0,114)
\definecolor{centralesupelec}{RGB}{150,14,59}   % rgb(150,14,59)
\definecolor{centralesupelecdark}{RGB}{84,8,33}   % rgb(84,8,33)
\definecolor{centralesupeleclight}{RGB}{204,19,80}   % rgb(204,19,80)

%%%%% BEAMER CUSTOMIZATION

\setbeamercolor{block body}{bg=blue!10!white}
\setbeamercolor{block body alerted}{bg=red!10!white}
\setbeamercolor{block body example}{bg=green!10!white}
\setbeamercolor{block title}{bg=blue!80!black,fg=white}
\setbeamercolor{block title alerted}{use={normal text,alerted text},fg=white,bg=red!80!black}
\setbeamercolor{block title example}{use={normal text,example text},fg=white,bg=green!50!black}
\setbeamercolor{item projected}{bg=blue!70!white,fg=white}

\setbeamertemplate{enumerate items}[ball]
\setbeamertemplate{enumerate subitem}{\insertenumlabel.\insertsubenumlabel}
\setbeamertemplate{enumerate subsubitem}{\insertenumlabel.\insertsubenumlabel.\insertsubsubenumlabel}
\setbeamertemplate{enumerate mini template}{\insertenumlabel}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{headline}{}

\setbeamersize{text margin left=1cm,text margin right=1cm}
\setbeamersize{text margin left=0.5cm,text margin right=0.3cm}


%%%%% customize blocks

\newenvironment<>{orangeblock}[1]{%
	\setbeamercolor{block title}{fg=white,bg=orange}%
	\setbeamercolor*{block body}{fg=black,bg=orange!5}
	\begin{block}#2{#1}}{\end{block}}

\newenvironment<>{redblock}[1]{%
	\setbeamercolor{block title}{fg=white,bg=red}%
	\setbeamercolor*{block body}{fg=black,bg=orange!5}
	\begin{block}#2{#1}}{\end{block}}

\newenvironment<>{colorblock}[1]{%
	\setbeamercolor{block title}{fg=white,bg=centralesupelec}%
	\setbeamercolor*{block body}{fg=black,bg=centralesupelec!5}
	\begin{block}#2{#1}}{\end{block}}

\newenvironment<>{darkblock}[1]{%
	\setbeamercolor{block title}{fg=white,bg=centralesupelecdark}%
	\setbeamercolor*{block body}{fg=black,bg=centralesupelecdark!5}
	\begin{block}#2{#1}}{\end{block}}

\newenvironment<>{lightblock}[1]{%
	\setbeamercolor{block title}{fg=white,bg=centralesupeleclight}%
	\setbeamercolor*{block body}{fg=black,bg=centralesupeleclight!5}
	\begin{block}#2{#1}}{\end{block}}


%%%%% customize titles

\setbeamertemplate{frametitle}{\centering\vspace{0.2cm}\color{blue}\insertframetitle\par\vspace{.2cm}}

\newcommand{\shadedtitle}[3]{
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	\begin{textblock}{12.6}(0.,0.75)
		\begin{tikzpicture}
		\node[top color=black,bottom color=white]
		{
			\begin{minipage}[t][0cm][b]{12.6cm}
			{.}
			\end{minipage}
		};
		\end{tikzpicture}
	\end{textblock}
	\begin{textblock}{12.6}(0,0)
		\begin{tikzpicture}
		\node[left color=#2,right color=#3]
		{
			\begin{minipage}[t][12pt][t]{12.6cm}
			{\color{white}#1}
			\end{minipage}
		};
		\end{tikzpicture}
	\end{textblock}
}

% \newcommand{\mytitle}[2]{\shadedtitle{\Large\bf #2}{#1}{#1!10!white}}
\newcommand{\mytitle}[3]{\shadedtitle{\Large\bf #3}{#1}{#1!10!#2}}

%%%%%%% define new frames

% Odalric's style frame
\NewEnviron{frameO}[1][]{%
\begin{frame}\mytitle{centralesupelecdark}{centralesupelec}{#1}

\vspace{0.4cm}

\BODY
\end{frame}
}

% frame for (main) titles
\NewEnviron{frameT}[1][]{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth,height=\paperheight]{../templateCS/PremierePage_CentraleSupelec}}
\setbeamertemplate{footline}{ \hspace{5em} \textcolor{white} {Lilian Besson - \emph{MAB Algorithms for IoT Networks} \hfill 17 October, 2019}\hspace{2em}\null \vspace*{3pt}}
\begin{frame}{#1}
\BODY
\end{frame}
}

\NewEnviron{frameTTnobottom}[1][]{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth,height=\paperheight]{../templateCS/PremierePage_CentraleSupelec}}
\setbeamertemplate{footline}{ \hspace{5em} \textcolor{white} {}\hspace{2em}\null \vspace*{3pt}}
\begin{frame}{#1}
\BODY
\end{frame}
}

\NewEnviron{frameTT}[1][]{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth,height=\paperheight]{../templateCS/PremierePage_CentraleSupelec}}
\setbeamertemplate{footline}{ \hspace{5em} \textcolor{white} {Lilian Besson - \emph{MAB Algorithms for IoT Networks}  \hfill 17 October, 2019}\hspace{2em}\null \vspace*{3pt}}
\begin{frame}{#1}
\BODY
\end{frame}
}

% frame for intermediate titles
\NewEnviron{frameTI}[1][]{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth,height=\paperheight]{../templateCS/PageTab_CentraleSupelec}}

\setbeamertemplate{footline}{\hspace{2cm} \raisebox{2.5ex}
	{{Lilian Besson - \emph{MAB Algorithms for IoT Networks}}}\hfill
	\raisebox{2.5ex}
	{{17 October, 2019 - \insertframenumber / \inserttotalframenumber \hspace{5mm} \null }}}
\begin{frame}{#1}
\BODY
\end{frame}
}


%%%%% mathematical symbols
\input{symbolsdef}

%%%%% XXX from my old template

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi

\ifxetex
\usepackage{fontspec}
\setmainfont[Ligatures=Historic]{TeX Gyre Pagella}
\newfontfamily\FiraCode{Fira Code}
\setmonofont[Contextuals={Alternate}]{Fira Code}
\newfontfamily\Fontify[Path = ../common/]{Fontify-Regular}
\else
\newcommand{\Fontify}{}
\fi

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

%%%%%%%% BEGINS HERE

\begin{document}

\begin{frameTTnobottom}

    \begin{textblock*}{12cm}(13mm,50mm)
        {\textcolor{white} {
        {\huge \textsc{Multi-players Bandit Algorithms for Internet of Things Networks}}
        \\
        \vspace*{10pt}% XXX ?
        {\large
        % Seminar of the \textbf{SCEE team}\\
        % at \textbf{CentraleSupélec}, campus de Rennes\\
        % Thursday $17$th of October, $2019$
        %
        Seminar of the \textbf{SequeL team}\\
        at \textbf{INRIA} Lille Nord Europe\\
        Friday $8$th of November, $2019$
        % %
        % \textbf{PhD defense}\\
        % at \textbf{CentraleSupélec}, campus de Rennes\\
        % Wednesday $20$th of November, $2019$
        }
        }}
    \end{textblock*}

    \vspace*{-4pt}
\end{frameTTnobottom}

%%% presentation for standard frames

\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth,height=\paperheight]{../templateCS/PageTabInverse_CentraleSupelec}}

\setbeamertemplate{footline}{\hspace{2cm} \raisebox{2.5ex}
    {\textcolor{white}{Lilian Besson - \emph{MAB Algorithms for IoT Networks}}}\hfill
    \raisebox{2.5ex}
    {\textcolor{white}{17 October, 2019 - \insertframenumber / \inserttotalframenumber \hspace{5mm} \null }}}


\begin{frameO}[Who am I ?]

    Hi, I'm Lilian\dots

    \begin{itemize}
        \item finishing my PhD, in telecommunication and machine learning
        \item under supervision of Prof. Christophe Moy\\
            at SCEE team, IETR \& CentraleSupélec in Rennes (France)
        \item and Dr. Émilie Kaufmann at SequeL team, in CNRS and Inria in Lille\\
        % \hfill{} \emph{Thanks to her for some of the slides material!}
    \end{itemize}

    \vspace*{5pt}

    \begin{itemize}
        \item Email : \href{https://perso.crans.org/besson/}{\textcolor{darkgreen}{\texttt{Lilian.Besson @ CentraleSupelec.fr \& Inria.fr}}}

        \item $\hookrightarrow$ \href{https://perso.crans.org/besson/}{{\textcolor{blue}{\texttt{perso.crans.org/besson/}}}} \& \href{https://GitHub.com/Naereen/}{{\textcolor{blue}{\texttt{GitHub.com/Naereen}}}}
    \end{itemize}

    \vfill{}
    % XXX manual inclusion of logos
    \begin{center}
        \includegraphics[height=0.20\textheight]{../common/LogoIETR.png}
        \includegraphics[height=0.22\textheight]{../common/LogoCS.png}
        \includegraphics[height=0.20\textheight]{../common/LogoInria.jpg}
    \end{center}
\end{frameO}


% \begin{frameO}[What did I study during my PhD ?]

%     \begin{darkblock}{Telecommunications technology\dots}
%         \hspace{5pt} $\hookrightarrow$ \alert{\textbf{wireless}} networks\dots

%         \vspace*{20pt}

%         \hspace{10pt} $\hookrightarrow$ networks with \alert{\textbf{decentralized}} access\dots

%         \vspace*{20pt}

%         \hspace{15pt} $\hookrightarrow$ \alert{\textbf{some/many}} wireless devices access a wireless network\\
%         \hspace{30pt} served from \alert{\textbf{one}} access point, in an unlicensed standard:\\
%         \hspace{30pt} the base station is \alert{\textbf{not}} affecting devices to radio resources\dots

%         \vspace*{20pt}

%         \hspace{35pt} $\hookrightarrow$ we focus on the case of \alert{\textbf{Internet of Things}} networks
%     \end{darkblock}

% \end{frameO}



\section{Spectrum issues in wireless networks}

\begin{frameTI}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Introduction:} }}
    \end{center}
    \vspace*{10pt}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Spectrum issues} }}
    \end{center}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{in wireless networks} }}
    \end{center}
    % \vspace*{-4pt}
\end{frameTI}


\begin{frameO}[Spectrum scarcity: there are not enough free bands]
    \begin{itemize}
        \item
        Wireless networks run on different frequencies
        \item
        most of which are already allocated to a fixed usage
        % \item
    \end{itemize}
    $\implies$ Almost all frequencies are registered and their usages are limited!
    \begin{center}
        \includegraphics[width=0.75\textwidth]{United_States_Frequency_Allocations_Chart_2016_The_Radio_Spectrum_3}
    \end{center}
    \hfill{}
    {\tiny \textcolor{gray}{United States of North America, Department of Commerce, \textcopyright{} 2016}}
\end{frameO}


\begin{frameO}[But\dots world-wide non homogeneous frequency usage]
    But\dots almost everywhere and at anytime in the world, some radio channels are not used! (in any standard)
    \begin{center}
        \includegraphics[width=0.80\textwidth]{scan_WiFi_maison}
    \end{center}
    \hfill{}
    {\tiny \textcolor{gray}{Lilian Besson, screenshot from my FreeBox control interface, \textcopyright{} 2019}}

    \vspace*{10pt}
    \textbf{What if we could dynamically use the (most) empty channels?}
\end{frameO}


\begin{frameO}[What did I study during my PhD ? ($1/2$)]

    % \begin{darkblock}{Telecommunications technology\dots}
    %     \hspace{5pt} $\hookrightarrow$ wireless networks\dots\\
    %     \hspace{10pt} $\hookrightarrow$ networks with decentralized access\dots\\
    %     \hspace{15pt} $\hookrightarrow$ SOME/MANY wireless devices access a wireless network\\
    %     \hspace{30pt} served from ONE access point, in an unlicensed standard:\\
    %     \hspace{30pt} the base station is NOT affecting devices to radio resources\dots\\
    %     \hspace{35pt} $\hookrightarrow$ we focus on the case of \emph{Internet of Things} networks
    % \end{darkblock}

    \begin{darkblock}{Telecommunications technology\dots}
        \hspace{5pt} $\hookrightarrow$ \alert{\textbf{wireless}} networks\dots

        \hspace{10pt} $\hookrightarrow$ networks with \alert{\textbf{decentralized}} access\dots

        \hspace{15pt} $\hookrightarrow$ \alert{\textbf{some/many}} wireless devices access a wireless network\\
        \hspace{30pt} served from \alert{\textbf{one}} access point, in an unlicensed standard:\\
        \hspace{30pt} the base station is \alert{\textbf{not}} affecting devices to radio resources\dots

        \hspace{35pt} $\hookrightarrow$ we focus on the case of \alert{\textbf{Internet of Things}} networks
    \end{darkblock}

    \pause

    \begin{center}
        % \centering
        \includegraphics[width=0.50\linewidth]{system_model1.eps}
        % \caption{In our system model, some dynamic devices (in the \textcolor{blue}{IoT network in blue}) transmit packets to a gateway and suffer from the interference generated by neighboring networks (in \textcolor{orange}{orange left/right}).}
        % \label{fig:41:system_model1}
    \end{center}
    % \hfill{}
    % \vspace*{-30pt}
    % {\tiny \textcolor{gray}{[Bonnefoi, Besson et al, CROWNCOM 2017], Ch.5.2}}

\end{frameO}



\begin{frameO}[What did I study during my PhD ? ($2/2$)]

    \begin{colorblock}{Main questions\dots?}
        \begin{itemize}
            \item
            Can the devices optimize their access to the radio resources\\
            in a \textbf{\alert{simple}}, \textbf{\alert{efficient}}, \textbf{\alert{automatic}} and \textbf{\alert{decentralized}} way?\\
            In a given location, and a given time, for a given radio standard\dots

            % \item
            % \alert{Can the devices learn \textbf{on their own} to communicate \textbf{more efficiently}?}
        \end{itemize}
    \end{colorblock}

    \vspace{10pt}
    \pause

    \begin{lightblock}{\textbf{Main solutions !}}
        Yes we can! By letting the radio devices become ``intelligent'' \\
        \hspace{5pt} $\hookrightarrow$ by using \emph{Machine Learning} algorithms\dots\\
        \hspace{10pt} $\hookrightarrow$ of the \emph{Reinforcement Learning} family\dots\\
        \hspace{15pt} $\hookrightarrow$ we focussed on \textbf{\alert{Multi-Armed Bandit}} algorithms\\
        \hspace{30pt} and we showed they are well suited for (some of) these problems!
    \end{lightblock}

\end{frameO}


\section{Outline of this talk}

\begin{frameTI}
    \begin{center}
        \color{white} \Huge \textsc{Outline of this talk}

        % \vspace{0.2cm}

        % \huge \textsc{and first strategies}

    \end{center}
    \vspace*{-4pt}
\end{frameTI}


% \begin{frameO}[Main contributions presented today]

%     \large

%     \begin{itemize}
%         \setlength\itemsep{10pt}
%         \item a simple model of IoT network, with decentralized learning embedded in the autonomous IoT devices,
%         \item numerical simulations proving the quality of the proposed solution,
%         \item a realistic implementation on radio hardware,
%         % \item my Python library SMPyBandits.
%         \item theoretical results in a simplified model: multi-player bandits,
%         \item and an extension beyond the stationary hypothesis.
%     \end{itemize}

% \end{frameO}


\begin{frameO}[Chapters of my thesis addressed today]

    \large

    \begin{enumerate}[leftmargin=50pt]
        \setlength\itemsep{10pt}
        % \item[Ch.1)] Introduction
        \item[Ch.2)] \textcolor{blue}{Stochastic Multi-Armed Bandits}
        \item[\textcolor{gray}{Ch.3)}] \textcolor{gray}{SMPyBandits: an exhaustive Python library to simulate MAB problems}
        \item[\textcolor{gray}{Ch.4)}] \textcolor{gray}{Expert aggregation for online MAB algorithms selection}
        \item[Ch.5)] \textcolor{blue}{Improving Spectrum Usage of IoT Networks with Selfish MAB Learning}
        \item[Ch.6)] \textcolor{blue}{Multi-Players Multi-Armed Bandits}
        \item[Ch.7)] \textcolor{blue}{Piece-Wise Stationary Bandits}
        % \item[Ch.8)] General Conclusion and Perspectives
    \end{enumerate}

\end{frameO}


\begin{frameO}[Organization of the thesis: our reading map for today]

    \begin{figure}[h!]
        \centering
        \resizebox{0.95\textwidth}{!}{
        \begin{tikzpicture}[>=latex',line join=bevel,scale=2.25]
            %
            \node[align=center] (introduction) at (0,3.25) [rectangle,draw,fill=blue!15] {\textbf{Chapter~1}\\Introduction};
            \node[align=center] (chapter2) at (0,2.25) [rectangle,draw,fill=red!15] {\textbf{Chapter~2}\\The Stochastic\\Multi-Armed Bandit models};
            \node[align=center] (chapter3) at (-2.5,2.25) [rectangle,draw,fill=gray!10,text=gray] {Chapter~3\\SMPyBandits: simulation\\library for MAB};
            \node[align=center] (chapter25) at (+2.5,2.25) [rectangle,draw,fill=gray!10,text=gray] {Chapter~4\\Online selection\\of the best algorithm};
            \node[align=center] (chapter4) at (-2.5,1) [rectangle,draw,fill=green!10] {\textbf{Chapter~5}\\Two MAB models\\for IoT networks};
            \node[align=center] (chapter5) at (0,1) [rectangle,draw,fill=green!15] {\textbf{Chapter~6}\\Multi-players\\Multi-Armed Bandits};
            \node[align=center] (chapter6) at (2.5,1) [rectangle,draw,fill=green!20] {\textbf{Chapter~7}\\Piece-Wise Stationary\\Multi-Armed Bandits};
            \node[align=center] (conclusion) at (0,-0.25) [rectangle,draw,fill=blue!20] {\textbf{Chapter~8}\\General Conclusion};
            % \node[align=center] (appendix) at (2.5,-0.25) [rectangle,draw,fill=yellow!10] {Appendix};
            %
            \draw [color=black,thick,->] (introduction) to (chapter2);
            % \draw [color=black,thick,<->] (chapter2) to (chapter3);
            % \draw [color=black,thick,<->] (chapter2) to (chapter25);
            \draw [color=black,thick,->] (chapter2) to (chapter4);
            % \draw [color=black,thick,->] (chapter2) to (chapter5);
            \draw [color=black,thick,->]   (chapter4) to (chapter5);
            % \draw [color=black,densely dotted,->] -| (chapter3) to (chapter25);
            % \draw [color=black,densely dotted,->] -| (chapter25) to (chapter6);
            \draw [color=black,thick,->]   (chapter5) to (chapter6);
            % \draw [color=black,thick,->] (chapter2) to (chapter6);
            % \draw [color=black,thick,->] (chapter4) to (conclusion);
            % \draw [color=black,thick,->] (chapter5) to (conclusion);
            \draw [color=black,thick,->] (chapter6) to (conclusion);
            % \draw [color=black,thick,->] (conclusion) to (appendix);
            %
        \end{tikzpicture}
        }
        % \caption[Organization of the thesis: a reading map.]{A reading map of the thesis. Any top-down path containing Chapter~\ref{chapter:1}, Chapter~\ref{chapter:2}, at least one of the three Chapters~\ref{chapter:4}, \ref{chapter:5} and \ref{chapter:6}, and the Conclusion is a self contained way to read this thesis.}
        \label{fig:1:organization}
    \end{figure}
\end{frameO}


\begin{frameO}[Outline of this talk]

    \begin{itemize}
        \setlength\itemsep{0.6em}
        \item \textcolor{gray}{Spectrum issues in wireless networks}
        \item Multi-armed Bandit
        \begin{itemize}
            \item \textcolor{gray}{applying bandit to Opportunistic Spectrum Access (OSA)}
            \item \textcolor{gray}{performance measure (regret) and first strategies}
            \item \textcolor{gray}{best possible regret? Lower bounds}
            \item \textcolor{gray}{upper Confidence Bounds (UCB) Algorithms}
        \end{itemize}
        \item Selfish MAB learning in our model of IoT network
        \begin{itemize}
            \item \textcolor{gray}{reference strategies}
            \item \textcolor{gray}{selfish applications of UCB algorithms}
            \item \textcolor{gray}{intractable model in theory\dots}
        \end{itemize}
        \item Two extensions of the stationary single-player bandit models
        \begin{itemize}
            \item \textcolor{gray}{multi-player bandits in stationary settings}
            \item \textcolor{gray}{single-player bandits in piece-wise stationary settings}
        \end{itemize}
        \item Conclusion and perspectives
    \end{itemize}

\end{frameO}



\section{Introduction to Multi-Armed Bandits}

\begin{frameTI}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Introduction to Multi-Armed Bandits} }}
    \end{center}
    \vspace*{-4pt}
\end{frameTI}

\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Introduction to Multi-Armed Bandits}
            \vspace{1.5cm}

            \textbf{\Large Why Bandits?}\\[0.5cm]
            \textcolor{darkgray}{\textbf{\Large Performance measure}\\[0.5cm] }
            \textcolor{darkgray}{\textbf{\Large Best possible regret?}\\[0.5cm] }
            \textcolor{darkgray}{\textbf{\Large The optimism principle}\\[0.5cm] }
        \end{center}
    \end{changemargin}
\end{frame}



\begin{frameO}[Hum, what is a \emph{bandit}?]
    \begin{center}
        It's an old name for a casino machine!
    \end{center}

    \begin{center}
        \includegraphics[height=6cm]{Lucky_Luke__Le_Bandit_Manchot.jpg}

        \begin{tiny}
            \textcolor{gray}{
                \textcopyright{} Dargaud,
                \href{https://www.dargaud.com/bd/LUCKY-LUKE/Lucky-Luke/Lucky-Luke-tome-18-Bandit-manchot-Le}{\textcolor{blue}{Lucky Luke tome 18}}.
            }
        \end{tiny}
    \end{center}
\end{frameO}


\begin{frameO}[Make money in a casino?]
    \begin{center}
        \includegraphics[height=3cm]{MABpieuvre}
    \end{center}
    \begin{center}
        A (single) \blue agent \black facing (multiple) \red arms \black in a Multi-Armed Bandit.
    \end{center}

    \pause
    \begin{center}
        \Huge NO!
    \end{center}
\end{frameO}

\begin{frameO}[Sequential resource allocation]
    \textbf{Clinical trials}
    \begin{itemize}
        \item $K$ treatments for a given symptom (with unknown effect)

              \includegraphics[width=0.12\linewidth]{medoc1.jpg}
              \hspace{0.05cm}
              \includegraphics[width=0.12\linewidth]{medoc4.jpg}
              \hspace{0.05cm}
              \includegraphics[width=0.12\linewidth]{medoc3.jpg}
              \hspace{0.05cm}
              \includegraphics[width=0.12\linewidth]{medoc2.jpg}
              \hspace{0.05cm}
              \includegraphics[width=0.12\linewidth]{medoc5.jpg}
              \hspace{0.05cm}
              \includegraphics[width=0.12\linewidth]{medoc6.jpg}
              \hspace{0.05cm}

        \item What treatment should be allocated to the next patient, based on responses observed on previous patients?
    \end{itemize}

    \vspace{0.2cm}

    \pause

    \textbf{Online advertisement}
    \begin{itemize}
        \item $K$ adds that can be displayed
              \vspace{0.1cm}

              \includegraphics[height=0.15\paperheight]{ad6.jpg}
              \hspace{0.1cm}
              \includegraphics[height=0.15\paperheight]{ad2.jpg}
              \hspace{0.1cm}
              \includegraphics[height=0.15\paperheight]{ad4.jpg}
              \hspace{0.1cm}
              \includegraphics[height=0.15\paperheight]{ad5.jpg}
              \hspace{0.05cm}

        \item Which add should be displayed for a user, based on the previous clicks of previous (similar) users?
    \end{itemize}

\end{frameO}



% \begin{frameO}[Dynamic allocation of computational resources]

%     \vspace{0.4cm}

%     \textbf{Numerical experiments} (bandits for ``black-box'' optimization)

%     \vspace{-0.3cm}

%     \begin{center}
%         \includegraphics[height=2.5cm]{GP}
%     \end{center}

%     \vspace{-0.3cm}

%     \begin{itemize}
%         \item where to evaluate a costly function in order to find its maximum?
%     \end{itemize}

%     \pause

%     \textbf{Artificial intelligence for games}

%     \begin{center}
%         \includegraphics[height=2.2cm]{MCTSWiki}
%     \end{center}

%     \vspace{-0.5cm}

%     \begin{itemize}
%         \item where to choose the next evaluation to perform in order to find the best move to play next?
%     \end{itemize}

% \end{frameO}


\begin{frameO}[Dynamic channel selection]

    \vspace{0.3cm}

    \textbf{Communications in presence of a central controller}
    \begin{itemize}
        \item $K$ assignments from $n$ users to $m$ antennas ($\rightsquigarrow$ \emph{combinatorial} bandit)

              \hspace{2.5cm}\includegraphics[height=0.2\paperheight]{assignements}
        \item How to select the next \emph{matching} based on the throughput observed in previous communications?
    \end{itemize}

    \vspace{0.1cm}

    \pause

    \textbf{Opportunistic Spectrum Access (OSA)}
    \begin{itemize}
        \item $K$ radio channels (orthogonal frequency bands)

              \hspace{0.4cm}\includegraphics[height=0.17\paperheight]{spectrum}
        \item In which channel should a radio device send a packet, based on the quality of its previous communications?
    \end{itemize}

\end{frameO}


%%% TITLE SLIDE FOR PART I


% standard slides for Part I

\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth,height=\paperheight]{../templateCS/PageTabInverse_CentraleSupelec}}

\setbeamertemplate{footline}{\hspace{2cm} \raisebox{2.5ex}
    {\textcolor{white}{Lilian Besson - \emph{MAB Algorithms for IoT Networks}}}\hfill
    \raisebox{2.5ex}
    {\textcolor{white}{17 October, 2019 - \insertframenumber / \inserttotalframenumber \hspace{5mm} \null }}}

\subsection{Multi-armed Bandit}

\begin{frameO}[\alt<2>{The \blue Stochastic \color{white} Multi-Armed Bandit Setup}{The Multi-Armed Bandit Setup}]

    \alt<2>{\vspace{0.4cm}}{}

    \begin{center}
        $K$ \textbf{arms} $\Leftrightarrow$ $K$ \alt<2>{\blue probability distributions \black : $\nu_a$ has mean $\blue\mu_a$}{rewards streams $(X_{a,t})_{t\in\N}$}
    \end{center}

    \begin{center}
        \includegraphics[height=0.15\textheight]{slot1.jpg}
        \hspace{0.4cm}
        \includegraphics[height=0.15\textheight]{slot2.jpg}
        \hspace{0.4cm}
        \includegraphics[height=0.15\textheight]{slot3.jpg}
        \hspace{0.4cm}
        \includegraphics[height=0.15\textheight]{slot4.jpg}
        \hspace{0.5cm}
        \includegraphics[height=0.15\textheight]{slot5.jpg}
        \hspace{0.4cm}
    \end{center}

    \vspace{-0.8cm}

    \[ \alt<2>{\blue\nu_1}{} \hspace{1.4cm} \alt<2>{\blue\nu_2}{} \hspace{1.4cm} \alt<2>{\blue\nu_3}{} \hspace{1.4cm} \alt<2>{\blue\nu_4}{} \hspace{1.4cm} \alt<2>{\blue\nu_5}{}\]

    \alt<2>{\vspace{-0.4cm}}{}

    At round $t$, an agent:
    \begin{itemize}
        \item chooses an  arm $A_t$
        \item receives a reward \alt<2>{$R_t = X_{A_t,t}\blue \overset{\text{iid}}{\sim} \nu_{A_t}$ (i.i.d. from a distribution)}{$R_t = X_{A_t,t}$ (from the environment)}
    \end{itemize}

    \vspace{0.2cm}

    \red Sequential \black sampling strategy (\textbf{bandit algorithm}):
    $\red A_{t+1} = F_t (A_1,R_1,\dots,A_{t},R_{t})\black$.

    \textbf{Goal:} Maximize sum of rewards \alt<2>{$\blue \bE\black\left[\sum\limits_{t=1}^T R_t\right]$}{$\sum\limits_{t=1}^T R_t$}.


\end{frameO}

\begin{frameO}[Discover bandits by playing this online demo!]

    \begin{center}
        \includegraphics[width=0.75\textwidth]{example_of_a_5_arm_bandit_problem.png}
    \end{center}

    % \begin{small}
    $\hookrightarrow$ Interactive demo on this web-page
    \href{https://perso.crans.org/besson/phd/MAB_interactive_demo/}{\textcolor{blue}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}}
    % Ref: [Bandits Algorithms, Lattimore \& Szepesv{\'a}ri, 2019],
    % on \href{https://tor-lattimore.com/downloads/book/book.pdf}{\textcolor{blue}{\texttt{tor-lattimore.com/downloads/book/book.pdf}}}
    % \end{small}

\end{frameO}

% \begin{frameO}[Clinical trials]

%     \textbf{Historical motivation} \color{gray}[Thompson 1933]\color{black}

%     \begin{center}
%         \includegraphics[width=0.12\linewidth]{medoc1.jpg}
%         \hspace{0.3cm}
%         \includegraphics[width=0.12\linewidth]{medoc4.jpg}
%         \hspace{0.3cm}
%         \includegraphics[width=0.12\linewidth]{medoc3.jpg}
%         \hspace{0.5cm}
%         \includegraphics[width=0.12\linewidth]{medoc2.jpg}
%         \hspace{0.5cm}
%         \includegraphics[width=0.12\linewidth]{medoc5.jpg}
%         \hspace{0.3cm}
%     \end{center}
%     \vspace{-0.8cm}

%     \hspace{-0.3cm}\[ \cB(\mu_1) \hspace{0.9cm} \cB(\mu_2) \hspace{0.9cm} \cB(\mu_3) \hspace{0.8cm} \cB(\mu_4) \hspace{0.9cm} \cB(\mu_5)\]

%     For the $t$-th patient in a clinical study,

%     \begin{itemize}
%         \item chooses a \blue treatment $A_t$\black
%         \item observes a (Bernoulli) \blue response $R_t \in \{0,1\} : \bP(R_t = 1 | A_t = a) = \mu_{a}$\black
%     \end{itemize}

%     \vspace{0.3cm}


%     \textbf{Goal:} maximize the expected number of patients healed.

% \end{frameO}



% \begin{frameO}[Online content optimization]

%     \textbf{Modern motivation} ($\$\$\$\$$) \gray [Li et al, 2010] \black

%     (recommender systems, online advertisement, etc)


%     \begin{center}
%         \includegraphics[height=0.15\textheight]{film1.jpg}
%         \hspace{0.6cm}
%         \includegraphics[height=0.15\textheight]{film2.jpg}
%         \hspace{0.6cm}
%         \includegraphics[height=0.15\textheight]{film3.jpg}
%         \hspace{0.6cm}
%         \includegraphics[height=0.15\textheight]{film4.jpg}
%         \hspace{0.6cm}
%         \includegraphics[height=0.15\textheight]{film5.jpg}
%         \hspace{0.6cm}
%     \end{center}

%     \vspace{-0.8cm}

%     \hspace{-0.2cm}\[ \nu_1 \hspace{1.4cm} \nu_2 \hspace{1.4cm} \nu_3 \hspace{1.4cm} \nu_4 \hspace{1.4cm} \nu_5\]

%     For the $t$-th visitor of a website,
%     \begin{itemize}
%         \item recommend a  \blue movie $A_t$\black
%         \item observe a \blue rating $R_t \sim \nu_{A_t}$\black \ (e.g. $R_t \in \{1,\dots,5\}$)
%     \end{itemize}

%     \vspace{0.3cm}

%     \textbf{Goal:} maximize the sum of ratings.

% \end{frameO}


\begin{frameO}[Application to Cognitive Radios: OSA]

    \textbf{Opportunistic Spectrum Access}
    \textcolor{gray}{
        % [Zhao et al. 2010]
        % [Anandkumar et al. 2011]
        [Jouini, Moy et al. 2010]
    }

    \begin{center}

        \emph{streams indicating channel quality}

        \vspace{0.3cm}

        \begin{tabular}{|c||c|c|c|c|c|c|c|}
            \hline
            Channel $1$ & \cellcolor{blue!25}$X_{1,1}$ & $X_{1,2}$                    & \dots & $X_{1,t}$                    & \dots & $X_{1,T}$                    & $\sim \nu_1$ \\
            \hline
            Channel $2$ & $X_{2,1}$                    & \cellcolor{blue!25}$X_{2,2}$ & \dots & $X_{2,t}$                    & \dots & \cellcolor{blue!25}$X_{2,T}$ & $\sim \nu_2$ \\
            \hline
            \dots       & \dots                        & \dots                        & \dots & \dots                        & \dots & \dots                        & \dots        \\
            \hline
            Channel $K$ & $X_{K,1}$                    & $X_{K,2}$                    & \dots & \cellcolor{blue!25}$X_{K,t}$ & \dots & $X_{K,T}$                    & $\sim \nu_K$ \\
            \hline
        \end{tabular}
    \end{center}

    \vspace{0.2cm}

    At round $t$, the device:
    \begin{itemize}
        \item selects \blue{a channel} $A_t \in \{1,\dots,K\}$\black
        \item observes the \blue quality of its communication  $R_t = X_{A_t,t} \in [0,1]$\black
        \begin{itemize}
            \item $R_T \in\{0,1\}$ binary reward: success/failure of message transmission
            \item $R_T \in[0,1]$ continuous reward: e.g., received power,  etc
        \end{itemize}
    \end{itemize}

    \vspace{0.2cm}

    \textbf{Goal:} Maximize the overall quality of communications.

\end{frameO}



\subsection{Performance measure and first strategies}

\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Introduction to Multi-Armed Bandits}
            \vspace{1.5cm}

            \textcolor{darkgray}{\textbf{\Large Why Bandits?}\\[0.5cm]}
            \textbf{\Large Performance measure and first strategies}\\[0.5cm]
            \textcolor{darkgray}{\textbf{\Large Best possible regret?}\\[0.5cm] }
            \textcolor{darkgray}{\textbf{\Large The optimism principle}\\[0.5cm] }
        \end{center}
    \end{changemargin}
\end{frame}


\begin{frameO}[Regret of a bandit algorithm]

    \bigskip

    \textbf{Bandit instance:} $\bm\nu = (\nu_1,\nu_2, \dots,\nu_K)$, mean of arm $a$: $\mu_a = \bE_{X \sim \nu_a}[X]$.

    \[\red\mu_\star = \max_{a \in \{1,\dots,K\}} \mu_a \ \ \text{and} \ \ \ a_\star = \argmax_{a \in \{1,\dots,K\}} \mu_a.\]
    \[\begin{array}{ccl}\text{Maximizing rewards} & \Leftrightarrow & \text{selecting } a_\star \text{ as much as possible }         \\
                                           & \Leftrightarrow & \text{minimizing the \blue regret } \gray \text{[Robbins, 52]}
        \end{array}\]

    \vspace{-0.4cm}

    \begin{eqnarray*}
        \blue \cR_{\bm\nu}(\cA,T) \eqdef \black\underbrace{\blue T \mu_\star}_{\substack{\text{sum of rewards of}\\ \text{an oracle strategy} \\ \text{always selecting } a_\star}} \blue- \black\underbrace{\blue\bE\left[\sum_{t=1}^{T}R_{t}\right]}_{\substack{\text{sum of rewards of}\\\text{the strategy} \cA}}
    \end{eqnarray*}

    \vspace{-0.4cm}
    \pause

    \begin{colorblock}{What regret rate can we achieve?}
        \begin{itemize}
            \item[$\Longrightarrow$] consistency \black $\cR_{\bm\nu}(\cA,T) / T \Longrightarrow 0$ (when $T\to\infty$)
            \item[$\Longrightarrow$] can we be more precise?
        \end{itemize}
    \end{colorblock}
\end{frameO}

\begin{frameO}[Regret decomposition]

    \vspace{0.6cm}

    $N_a(t)$ : number of selections of arm $a$ in the first $t$ rounds

    $\Delta_a \eqdef \mu_\star -\mu_a$ : sub-optimality gap of arm $a$

    \begin{colorblock}{Regret decomposition}
        \[\alert<1>{ \cR_{\bm\nu}(\cA,T) = \sum_{a=1}^K \Delta_a \bE\left[N_a(T)\right] }.\]
    \end{colorblock}
    \pause

    \vspace{0.6cm}

    A strategy with small regret should:
    \begin{itemize}
        \item select not too often arms for which $\Delta_a > 0$ (sub-optimal arms)
        \item \dots which requires to try all arms to estimate the values of the $\Delta_a$
    \end{itemize}

    \vspace{0.5cm}

    \begin{center}
        \red $\Longrightarrow$ Exploration / Exploitation trade-off !
    \end{center}

\end{frameO}


\begin{frameO}[Two naive strategies]

    \vspace{0.4cm}

    \begin{itemize}
        \item \textbf{Idea 1 :}
              \hfill{} \color{red}$\Longrightarrow$ EXPLORATION \color{black}
              \begin{colorblock}{}Draw each arm $T/K$ times\end{colorblock}
    \end{itemize}

    \vspace{-0.3cm}

    \[\hookrightarrow \cR_{\bm\nu}(\cA,T) = \left(\frac{1}{K}\sum_{a : \mu_a > \mu_\star} \Delta_a\right) T = \alert{\Omega(T)} \]

    \pause

    \begin{itemize}
        \item \textbf{Idea 2 :} Always trust the empirical best arm
              \hfill{} \color{red}$\Longrightarrow$ EXPLOITATION \color{black}
    \end{itemize}
    \begin{colorblock}{}
        $A_{t+1}=\underset{a \in \{1,\dots,K\}}{\text{argmax}} \ \blue \widehat{\mu}_a(t)$
        using estimates of the unknown means $\mu_a$

        \[\blue \widehat{\mu}_a(t) =\frac{1}{N_a(t)}\sum_{s=1}^t X_{a,s} \ind_{(A_s=a)}\]
    \end{colorblock}

    \vspace{-0.3cm}

    \[\hspace{1.5cm} \hookrightarrow \cR_{\bm\nu}(\cA,T) \geq  (1-\mu_1)\times \mu_2 \times (\mu_1 - \mu_2) T  = \alert{\Omega(T)} \]
    \begin{center}\vspace{-0.2cm}
        \hspace{1.5cm}(with $K=2$ Bernoulli arms of means $\mu_1 \neq \mu_2$)
    \end{center}


\end{frameO}



\subsection{Best possible regret? Lower bounds}

\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Introduction to Multi-Armed Bandits}
            \vspace{1.5cm}

            \textcolor{darkgray}{\textbf{\Large Why Bandits?}\\[0.5cm]}
            \textcolor{darkgray}{\textbf{\Large Performance measure and first strategies}\\[0.5cm]}
            \textbf{\Large Best possible regret? Lower bounds}\\[0.5cm]
            \textcolor{darkgray}{\textbf{\Large The optimism principle}\\[0.5cm] }
        \end{center}
    \end{changemargin}
\end{frame}


\begin{frameO}[The Lai and Robbins lower bound]

    \vspace{0.3cm}

    \textbf{Context:} a \blue parametric bandit model \black where each arm is parameterized by its mean $\bm\nu =(\nu_{\mu_1},\dots,\nu_{\mu_K})$, $\mu_a \in \cI$.
    \[ \text{distributions } \bm\nu \ \ \Leftrightarrow \ \ \bm\mu = (\mu_1,\dots,\mu_K) \text{ means} \]


    \textbf{Key tool:} \blue Kullback-Leibler divergence\black.

    \begin{colorblock}{Kullback-Leibler divergence}

        \alt<3>{\vspace{-0.2cm}}{}

        \[
            \red \mathrm{kl}(\mu,\mu')  \black : =   \alt<3>{\mu\log \left(\frac{\mu}{\mu'}\right) + (1-\mu) \log \left(\frac{1-\mu}{1-\mu'}\right) \ \ \ (\text{Bernoulli bandits})}{\alt<2>{\frac{(\mu - \mu')^2}{2\sigma^2} \ \ \ (\text{Gaussian bandits with variance } \sigma^2)}{\text{KL}\left(\nu_\mu,\nu_{\mu'}\right) =\bE_{X \sim \nu_{\mu}}\left[\log \frac{d\nu_{\mu}}{d\nu_{\mu'}}(X)\right]}}
        \]


        \vspace{-0.3cm}

    \end{colorblock}
    \begin{colorblock}{Theorem \hfill{} \textcolor{gray}{[Lai and Robbins, 1985]}}
        For uniformly efficient algorithms ($\cR_{\bm\mu}(\cA,T)=\mathcal{o}(T^\alpha)$ for all $\alpha\in (0,1)$ and $\bm \mu \in \cI^K$),

        \vspace{-0.4cm}

        \[
            \color{red}\mu_a<\mu_\star \Longrightarrow \liminf_{T\rightarrow\infty}\frac{\bE_{\bm \mu}[N_{a}(T)]}{\log T}\geq \frac{1}{\mathrm{kl}(\mu_a,\mu_\star)}.\color{black}
        \]

        \vspace{-0.3cm}

    \end{colorblock}

\end{frameO}


\begin{frameO}[Other asymptotic lower bounds]

    Such asymptotic lower bound can be generalized to extensions of the stationary single-player MAB model
    $ \cR(\cA, T) \geq \Omega(C_{\text{problem}} K \log(T)) $.

    \vspace*{2pt}

    \begin{colorblock}{For \textbf{multi-player bandits} \hfill{} (with $2 \leq M \leq K$ players)}
        Same lower-bound, with a problem-dependent constant $C'_{\text{problem}}$
        \[ \cR(\cA, T) \geq \Omega(C'_{\text{problem}} M K \log(T)). \]
    \end{colorblock}

    \vspace*{2pt}

    \begin{colorblock}{For \textbf{piece-wise stationary bandits} \hfill{} (with $\Upsilon_T$ stationary intervals)}
        Larger lower-bound, with a problem-dependent constant $C''_{\text{problem}}$
        \[ \cR(\cA, T) \geq \Omega(C''_{\text{problem}} \sqrt{K \Upsilon_T T}). \]
    \end{colorblock}

    \textcolor{gray}{(our proposed algorithms are asymptotically matching the lower-bounds)}

\end{frameO}



\subsection{The Optimism Principle and Upper Confidence Bounds Algorithms}

\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Introduction to Multi-Armed Bandits}
            \vspace{1.5cm}

            \textcolor{darkgray}{\textbf{\Large Why Bandits?}\\[0.5cm]}
            \textcolor{darkgray}{\textbf{\Large Performance measure and first strategies}\\[0.5cm]}
            \textcolor{darkgray}{\textbf{\Large Best possible regret? Lower bounds}\\[0.5cm]}
            \textbf{\Large The optimism principle:\\
                Upper Confidence Bounds Algorithms}\\[0.5cm]
        \end{center}
    \end{changemargin}
\end{frame}


\begin{frameO}[A first UCB algorithm]

    \vspace{0.3cm}

    \begin{itemize}
        \item $N_a(t) = \sum_{s=1}^t\ind_{(A_s = a)}$ number of selections of $a$ after $t$ rounds
        \item $\hat \mu_{a,s} = \frac{1}{s}\sum_{k=1}^s Y_{a,k}$ average of the first $s$ observations from arm $a$
        \item $\widehat{\mu}_a(t) = \widehat{\mu}_{a,N_a(t)}$ empirical estimate of $\mu_a$ after $t$ rounds
    \end{itemize}

    \vspace{0.3cm}

    UCB($\alpha$) selects  $A_{t+1} = \argmax_a \ \mathrm{UCB}_a(t)$
    where
    \[\blue \mathrm{UCB}_a(t) =\black \underbrace{\blue \widehat{\mu}_{a}(t)}_{\text{exploitation term}} + \underbrace{\blue\sqrt{\frac{\alpha\log(t)}{N_a(t)}}}_{\text{exploration bonus}}.\]

    \vspace{0.3cm}

    \begin{colorblock}{Hoeffding inequality + union bound \hfill{} \textcolor{gray}{[Auer et al. 2002]}}
        \[ \bP\left(\mu_a \leq \red \widehat{\mu}_a(t) + \sigma\sqrt{\frac{\alpha\log(t)}{N_a(t)}} \black \right) \geq 1 - \frac{1}{t^{\frac{\alpha}{2} -1}} \]
    \end{colorblock}

    % \pause
    % \textbf{Proof.}

    % \vspace{-0.5cm}

    % \begin{align*}
    %      & \bP\left(\mu_a > \widehat{\mu}_a(t) + \sigma\sqrt{\frac{\alpha\log(t)}{N_a(t)}} \black \right) \leq  \bP\left(\exists s \leq t : \mu_a > \widehat{\mu}_{a,s} + \sigma\sqrt{\frac{\alpha\log(t)}{s}} \black\right) \\
    %      & \leq \sum_{s=1}^t \bP\left(\widehat{\mu}_{a,s} < \mu_a - \sigma\sqrt{\frac{\alpha\log(t)}{s}}\right) \leq \sum_{s=1}^t \frac{1}{t^{\alpha/2}} = \frac{1}{t^{\alpha/2 - 1}}.
    % \end{align*}


\end{frameO}


% \begin{frameO}[A first UCB algorithm]

%     \vspace{0.4cm}

%     UCB($\alpha$) selects  $A_{t+1} = \argmax_a \ \mathrm{UCB}_a(t)$
%     where
%     \[\blue \mathrm{UCB}_a(t) =\black \underbrace{\blue \widehat{\mu}_{a}(t)}_{\text{exploitation term}} + \underbrace{\blue\sqrt{\frac{\alpha\log(t)}{N_a(t)}}}_{\text{exploration bonus}}.\]

%     \begin{itemize}
%         \item this form of UCB was first proposed for Gaussian rewards

%               \gray [Katehakis and Robbins, 95]\black
%         \item popularized by \color{gray}[Auer et al. 02] \color{black} for bounded rewards:\\
%               \red UCB1, for $\alpha=2$\black

%         \item the analysis was UCB($\alpha$) was further refined to hold for $\alpha > 1/2$ in that case \gray [Bubeck, 11, Cappé et al. 13] \black
%     \end{itemize}

% \end{frameO}

\begin{frameO}[A UCB algorithm in action \textcolor{gray}{[Kaufmann, 2014]} \hfill{} (movie)]

    \begin{center}
        \movie{\includegraphics[width=0.9\textwidth]{KLUCB.pdf}}{KLUCB.avi}
    \end{center}
\end{frameO}


% \subsection{Analysis of UCB($\alpha$)}

% \begin{frame}[c]
%     \begin{changemargin}{-0.5cm}{-0.5cm}
%         \begin{center}
%             \vspace{-0.3in}
%             \textbf{\huge Optimistic Algorithms}
%             \vspace{1.5cm}

%             \textcolor{darkgray}{\textbf{\Large Building Confidence Intervals}	\\[0.5cm]}
%             {\textbf{\Large Analysis of UCB($\alpha$)}\\[0.5cm]		}
%             % \textcolor{darkgray}{\textbf{\Large Other UCB algorithms}\\[0.5cm]}
%         \end{center}
%     \end{changemargin}
% \end{frame}


\begin{frameO}[Regret of UCB($\alpha$) for bounded rewards]

    \vspace{0.3cm}

    \begin{colorblock}{Theorem \hfill{} \gray[Auer et al, 02]\black}
        UCB($\alpha$) with parameter $\alpha=2$ satisfies

        \vspace{-0.4cm}

        \[\cR_{\bm\nu}(\texttt{UCB1},T) \leq 8 \left(\sum_{a : \mu_a < \mu_\star} \frac{1}{\Delta_a}\right)\log(T) + (1+\frac{\pi^2}{3})\left(\sum_{a=1}^K \Delta_a\right).\]
    \end{colorblock}

    \vspace{1cm}

    In some of our works, we use an extension of UCB called kl-UCB.

    % FIXME talk about kl-UCB: just one block saying it works similarly but it is smarter?
    \begin{colorblock}{Theorem \hfill{} \gray[Garivier et al, 2013]\black}
        For bounded rewards, kl-UCB satisfies $\cR_{\bm\nu}(\texttt{kl-UCB},T) = \mathcal{O}(K C \log(T))$:

        \vspace{-0.4cm}

        \[\cR_{\bm\nu}(\texttt{kl-UCB},T) \leq \left(\sum_{a : \mu_a < \mu_\star} \frac{\Delta_a}{\texttt{kl}(\mu_a, \mu_\star)}\right)\log(T) + \mathcal{o}(\log(T)).\]
    \end{colorblock}


    % \begin{itemize}
    %     \item[$\Longrightarrow$] what we will prove today
    % \end{itemize}


    % \begin{colorblock}{Theorem} For every $\alpha>1$ and every sub-optimal arm $a$, there exists a constant $C_\alpha>0$ such that

    %     \vspace{-0.8cm}

    %     \[\bE_{\bm \mu}[N_a(T)] \leq \frac{4\alpha}{(\mu_\star - \mu_a)^2}\log(T) + C_\alpha.\]
    % \end{colorblock}

    % \vspace{0.2cm}

    % It follows that

    % \vspace{-0.6cm}

    % \[\cR_{\bm\nu}(\mathrm{UCB}(\alpha),T) \leq 4\alpha\blue\left(\sum_{a : \mu_a < \mu_\star}\frac{1}{\Delta_a}\right)\black\log(T) + K C_\alpha\black.\]

\end{frameO}


\section{Selfish MAB learning in IoT Networks}

\begin{frameTI}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Part I} }}
    \end{center}
    \vspace*{10pt}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Selfish Multi-Armed Bandit Learning in Internet-of-Things Networks} }}
    \end{center}
    % \vspace*{-4pt}
    % \vfill{}
    % \small{\textcolor{lightgray}{Ref: Chapter 4 of my thesis, and [Bonnefoi et al, 2017].}}
\end{frameTI}

\input{crowncom2017.tex}


\begin{frameO}[We implemented this with real hardware ($1/3$)]

    We developed a realistic demonstration using USRP boards and GNU Radio,
    as a proof-of-concept in a ``toy'' IoT network.

    \begin{center}
        \movie{\includegraphics[width=0.6\textwidth]{screenshotDemoYouTube.png}}{videos/demoICT2018.mkv}
    \end{center}
    {\tiny \textcolor{gray}{[Bonnefoi et al, ICT 2018], [Besson et al, WCNC 2019], Ch.5.3}}
\end{frameO}


\begin{frameO}[We implemented this with real hardware ($2/3$)]
    \begin{center}
        \movie{\includegraphics[width=0.9\textwidth]{screenshotDemoYouTube_Partie1.png}}{videos/Partie1.mp4}
    \end{center}
\end{frameO}


\begin{frameO}[We implemented this with real hardware ($3/3$)]
    \begin{center}
        \movie{\includegraphics[width=0.9\textwidth]{screenshotDemoYouTube_Partie2.png}}{videos/Partie2.mp4}
    \end{center}
\end{frameO}


\section{Extensions with Bandit Models}

\begin{frameTI}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Part II} }}
    \end{center}
    \vspace*{10pt}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Extensions with Bandit Models} }}
    \end{center}
    \vspace*{-4pt}
\end{frameTI}


\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Extensions with Bandit Models}
            \vspace{1.5cm}

            \Large \textbf{Many different extensions}	\\[0.5cm]
            \textcolor{darkgray}{\textbf{\Large Multi-player bandits}\\[0.5cm]}
            \textcolor{darkgray}{\textbf{\Large Piece-wise stationary bandits}\\[0.5cm] }
        \end{center}
    \end{changemargin}
\end{frame}

\begin{frameO}[Many other bandits models and problems (1/2)]

    Most famous extensions:

    \begin{itemize}[<+->]
        \setlength\itemsep{1em}
        \item (centralized) multiple-actions
              \begin{itemize}
                  \item \textcolor{orange}{multiple choice} : choose $m\in\{2,\dots,K-1\}$ arms (fixed size)
                  \item combinatorial : choose a subset of arms $S \subset \{1,\dots,K\}$ (large space)
              \end{itemize}
        \item non stationary
              \begin{itemize}
                  \item \textcolor{orange}{piece-wise stationary / abruptly changing}
                  \item slowly-varying, rotting\dots
                  \item adversarial\dots
              \end{itemize}
        \item (decentralized) collaborative/communicating bandits over a graph
        \item \textcolor{orange}{(decentralized) non communicating multi-player bandits}
    \end{itemize}

    % \hspace*{5cm}
    \vfill{}
    \hfill{}
    \textcolor{orange}{$\hookrightarrow$ Implemented in our library \textbf{SMPyBandits}!}

\end{frameO}

\begin{frameO}[Many other bandits models and problems (2/2)]

    And many more extensions\dots

    \begin{itemize}%[<+->]
        \setlength\itemsep{0.8em}
        \item non stochastic, Markov models rested/restless
        \item best arm identification (vs reward maximization)
              \begin{itemize}
                  \item fixed budget setting
                  \item fixed confidence setting
                  \item PAC (probably approximately correct) algorithms
              \end{itemize}
        \item bandits with (differential) privacy constraints
        \item for some applications (content recommendation)
              \begin{itemize}
                  \item contextual bandits : observe a reward and a \emph{context} ($C_t \in\mathbb{R}^d$)
                  \item cascading bandits
                  \item delayed feedback bandits
              \end{itemize}
        \item structured bandits (low-rank, many-armed, Lipschitz etc)
        \item $\mathcal{X}$-armed, continuous-armed bandits
    \end{itemize}

\end{frameO}



\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Other Bandit Models}
            \vspace{1.5cm}

            \textcolor{darkgray}{\Large \textbf{Many different extensions}	\\[0.5cm]}
            % \textcolor{darkgray}{\Large \textbf{Insights from the Optimal Solution}}	\\[0.5cm]
            \textbf{\Large Multi-player bandits}\\[0.5cm]
            \textcolor{darkgray}{\textbf{\Large Piece-wise stationary bandits}\\[0.5cm]}
        \end{center}
    \end{changemargin}
\end{frame}


\subsection{Bandits for multiple devices}

\begin{frameO}[Multi-players bandits: setup]

    \orange $M$ players \black playing \textit{the same} $K$-armed bandit
    \hfill{} ($2 \leq M \leq K$)
    % with \textbf{no communication} between them

    \vspace{0.3cm}

    At round $t$:
    \begin{itemize}
        \item player $m$ selects $A_{m,t}$ ; then \textit{observes} $X_{A_{m,t},t}$
        \item and receives the reward
    \end{itemize}

    \vspace{-0.2cm}

    \[\red X_{m,t} = \left\{ \begin{array}{cl}
            X_{A_{m,t},t} & \text{if no other player chose the same arm} \\
            0             & \text{else (= collision)}
        \end{array}\right.\]

    \vspace{-0.3cm}

    \textbf{Goal:}

    \begin{itemize}
        \item maximize centralized rewards $\sum\limits_{m=1}^M \sum\limits_{t=1}^T X_{m,t}$
        \item \dots \orange without communication \black between players
        \item trade off : exploration / exploitation / \alert{and collisions !}
    \end{itemize}

    % \underline{\orange Cognitive radio}: (OSA) sensing, attempt of transmission if no PU, possible collisions with other SUs

\end{frameO}

\begin{frameO}[Multi-players bandits for Cognitive Radios]

    \begin{colorblock}{Listen before talk}
        \begin{itemize}
            \item
            \underline{\orange Opportunistic Spectrum Access}: (OSA)
            \hfill{}
            {\tiny \textcolor{gray}{[Zhao et al. 2010], [Jouini et al. 2010], [Anandkumar et al. 2011]}}

            \item
            Sensing, attempt of transmission if no PU, possible collisions with other SUs.

            \item
            Feedback model: observe first $X_{A_{m,t}, t}$, transmit if $X_{A_{m,t}, t} = 1$ then observe $X_{m,t}$.
        \end{itemize}
    \end{colorblock}

    \begin{colorblock}{Talk and maybe collide}
        \begin{itemize}
            \item
            \underline{\orange Internet of Things}: (IoT)
            \hfill{}
            {\tiny \textcolor{gray}{[Besson \& Kaufmann, ALT 2018]}}

            \item
            no sensing, transmit and wait for an acknowledgment before next message.

            \item
            Feedback model: observe only $X_{m, t}$.\\
            If $\neq 0$, no collision, but if $= 0$, what happened? collision or zero reward?
        \end{itemize}
    \end{colorblock}

    FIXME finish this slide!

    \begin{colorblock}{Observe collision then talk?}
        \begin{itemize}
            \item
            A third model, studied by a paper following our work
            \textcolor{gray}{[Boursier et al, 2019]}

            \item
            First check if collision, then receive reward
            \alert{(unrealistic from cognitive radio)}
        \end{itemize}
    \end{colorblock}

\end{frameO}


\begin{frameO}[Multi-players bandits: algorithms]

    \only<1-2>{
        \textbf{Idea:} combine a good \textit{bandit algorithm} with an \emph{orthogonalization strategy} (collision avoidance protocol)

        \vspace{0.2cm}
    }

    \only<1>{
        \begin{colorblock}{\textbf{Old idea:} UCB1 + $\rho^{\text{rand}}$. At round $t$ each player $m\in\{1,\dots,M\}$}
            \begin{itemize}
                \item has a stored rank $R_{m,t} \in \{1,\dots,M\}$
                \item selects the arm that has \red the $R_{m,t}$-largest UCB\black
                \item if a collision occurs, draws a new rank $R_{m,t+1} \sim \cU(\{1,\dots,M\})$
                \item any index policy may be used in place of UCB1
                \item nice $\cR(\cA, T) = \mathcal{O}(M^2 K \log(T))$ regret bound\dots\\
                    but \alert{their proof was wrong!}
                \item \textbf{Early references}: \color{gray} [Liu and Zhao, 10] [Anandkumar et al., 11]\black
            \end{itemize}
        \end{colorblock}
    }

    \only<2->{
        \begin{colorblock}{\textbf{Our algorithm:} klUCB index + MC-TopM rule}
            \begin{itemize}
                \only<2>{
                \item more complicated behavior (musical chair game)
                \item we obtain a $\cR(\cA, T) = \mathcal{O}(M^3 \frac{1}{\Delta_M} \log(T))$ regret upper bound
                \item lower bound is $\cR(\cA, T) \geq \Omega(M \frac{1}{\Delta_M} \log(T))$\\
                    ($\equiv$ as easy as centralized multiple-play bandits)
                \item our algorithm is order optimal, not asymptotically optimal
                }
                \item \textbf{Recent references}: \color{gray} [Besson \& Kaufmann, ALT 2018] [Boursier et al, 19]
            \end{itemize}
        \end{colorblock}
    }

    \only<3>{
        \textbf{Remarks}:
        \begin{itemize}
            \item the number of players $M$ has to be known\\
                $\Longrightarrow$ but it is possible to estimate it on the run
            \item does not handle an evolving number of devices\\
                (entering/leaving the network)
            \item is it a \textit{fair} orthogonalization rule? (yes!)
            \item could players use the collision indicators to communicate? (yes!)\\
                $\implies$ \textbf{\textsc{Sic-MMAB}} strategy matches the centralized multiple-play lower-bound,
                by using collision indicators to build a communication protocol between devices (\emph{in another feedback model})
                \textcolor{gray}{[Boursier et al, 19]}
        \end{itemize}
    }

\end{frameO}

\begin{frameO}[Results on a multi-player MAB problem ($1/3$)]

    \centering
    \includegraphics[height=0.75\textheight]{MP__K9_M9_T10000_N200__4_algos/all_RegretCentralized____env1-1_2306423191427933958.pdf}
    \caption{\footnotesize{For $K=M$ objects, our strategy (MC-TopM) achieves \textbf{constant} regret!}}

\end{frameO}

\begin{frameO}[Results on a multi-player MAB problem ($2/3$)]

    \centering
    \includegraphics[height=0.75\textheight]{MP__K9_M6_T5000_N500__4_algos/all_RegretCentralized____env1-1_8318947830261751207.pdf}
    \caption{\footnotesize{For $M=6$ objects, our strategy (MC-TopM) largely outperform $\rho^{\text{rand}}$ and other previous state-of-the-art.}}

\end{frameO}

\begin{frameO}[Results on a multi-player MAB problem ($3/3$)]

    \centering
    \includegraphics[height=0.80\textheight]{all_RegretCentralized_loglog____env1-1_6747959631471381163.pdf}
    \caption{\footnotesize{For $M=6$ objects, our strategy (MC-TopM) largely outperform \textsc{Sic-MMAB} and $\rho^{\text{rand}}$. \textcolor{cyan}{MCTopM + klUCB} achieves the best performance (among decentralized algorithms) !}}

\end{frameO}

\begin{frame}[shrink=50]
    \mytitle{centralesupelecdark}{centralesupelec}{State-of-the-art multi-player algorithms}

    \vspace*{2cm}

    \begin{tabular}{l | c | c | c | c}
        \textbf{Algorithm} & \textbf{Paper} & \textbf{Regret bound} $\cR(\cA, T)$ & \textbf{Complexity} & \textbf{Parameters} \\
        \hline
        Centralized multiple-play UCB & \textcolor{gray}{[Anantharam et al. 1987]} & $\mathcal{O}( \sum\limits_{k: \mu_k < \mu_{M}^*} \sum\limits_{j=1}^M \frac{\mu_{M}^*}{\mathrm{kl}(\mu_k,\mu_j^*)} \log(T))$ & {\Large \textcolor{orange}{$\star$}} & $M$ (but other model) \\
        \hline
        $\rho^{\text{rand}}$ UCB & \textcolor{gray}{[Anandkumar et al. 2011]} & $\mathcal{O}(\log(T))$ & {\Large \textcolor{orange}{$\star$}} & $M$ \\
        MEGA & \textcolor{gray}{[Avner \& Mannor, 2015]} & $\mathcal{O}(T^{3/4})$ & {\Large \textcolor{orange}{$\star$}} & 4 parameters, impossible to tune \\
        Musical-Chair & \textcolor{gray}{[Rosenski et al. 2015]} & $\mathcal{O}(\log(T))$ & {\Large \textcolor{orange}{$\star$}} & $T_0$ impossible to tune \\
        Selfish UCB & \textcolor{gray}{[Bonnefoi et al. 2017]} & $\mathcal{O}(T)$ BAD in some cases & {\Large \textcolor{orange}{$\star$}} & none \\
        \hline
        \textcolor{blue}{MCTopM klUCB} & \textcolor{gray}{[Besson \& Kaufmann, 2018]} & $\mathcal{O}( M^2 \sum\limits_{k: \mu_k < \mu_{M}^*} \sum\limits_{j=1}^M \frac{\mu_{M}^*}{\mathrm{kl}(\mu_k,\mu_j^*)} \log(T))$ & {\Large \textcolor{orange}{$\star$}} & $M$ \\
        \hline
        \textsc{Sic-MMAB} & \textcolor{gray}{[Boursier et al. 2019]} & $\mathcal{O}( \sum\limits_{k: \mu_k < \mu_{M}^*} \sum\limits_{j=1}^M \frac{\mu_{M}^*}{\mathrm{kl}(\mu_k,\mu_j^*)} \log(T))$ & {\Large \textcolor{orange}{$\star\star\star$}} & none
    \end{tabular}

\end{frame}


\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge Other Bandit Models}
            \vspace{1.5cm}

            \textcolor{darkgray}{\Large \textbf{Many different extensions}\\[0.5cm]}
            % \textcolor{darkgray}{\Large \textbf{Insights from the Optimal Solution}}	\\[0.5cm]
            \textcolor{darkgray}{\textbf{\Large Multi-player bandits}\\[0.5cm]}
            \textbf{\Large Piece-wise stationary bandits}\\[0.5cm]
        \end{center}
    \end{changemargin}
\end{frame}


\subsection{Piece-wise stationary bandits}

\begin{frameO}[Piece-wise stationary bandits]

    \begin{colorblock}{Stationary MAB problems}
        Arm $a$ gives rewards sampled from \textcolor{blue}{the same distribution} for any time step
        \[ \forall t, r_a(t) \overset{\text{iid}}{\sim} \nu_a = \mathcal{B}(\textcolor{blue}{\mu_a}). \]
    \end{colorblock}

    \pause
    \begin{alertblock}{Non stationary MAB problems?}
        \alert{(possibly) different distributions} for any time step !
        \[ \forall t, r_a(t) \overset{\text{iid}}{\sim} \nu_a\alert{(t)} = \mathcal{B}(\mu_a\alert{(t)}). \]
    \end{alertblock}

    $\implies$ harder problem!
    And very hard if $\mu_a(t)$ can change at any step!

    \pause
    \begin{colorblock}{\textbf{Piece-wise stationary} problems!}
        $\hookrightarrow$ the literature usually focuses on the easier case,
        when there are at most $\Upsilon_T = \mathcal{o}(\sqrt{T})$ intervals, on which the means are all stationary.
    \end{colorblock}
\end{frameO}

\begin{frameO}[Example of a piece-wise stationary MAB problem]
    We plots the means \textcolor{red}{$\mu_1(t)$}, \textcolor{green}{$\mu_2(t)$}, \textcolor{blue}{$\mu_3(t)$}
    of $K=3$ arms.
    There are $\Upsilon_T=4$ break-points and $5$ sequences
    between $t=1$ and $t=T=5000$:
    \begin{center}
        \includegraphics[width=0.97\textwidth]{Problem_1.pdf}
    \end{center}
\end{frameO}

\begin{frameO}[Regret for piece-wise stationary bandits]

    The ``oracle'' algorithm plays the (unknown) best arm $k^*(t) = \argmax \mu_k(t)$
    (which changes between the $\Upsilon_T \geq 1$ stationary sequences)
    %
    \[ \alert{ \cR(\cA,T) } = \mathbb{E}\left[ \sum\limits_{t=1}^T r_{\alert{k^*(t)}}(t) \right] - \sum\limits_{t=1}^T \mathbb{E}\left[ r(t) \right] = \left(\alert{\sum_{t=1}^T \max_k \mu_k(t)} \right) - \sum\limits_{t=1}^T \mathbb{E}\left[ r(t) \right]. \]

    \pause
    \vspace*{10pt}

    \begin{colorblock}{Typical regimes for piece-wise stationary bandits}
        \begin{itemize}
            \item
                  The lower-bound is
                  $\cR(\cA,T) \geq \Omega(\sqrt{K T \Upsilon_T})$

            \item
                  Currently, state-of-the-art algorithms $\mathcal{A}$ obtain
                  \begin{itemize}
                    % \tightlist
                      \item
                            $\cR(\cA,T) \leq \mathcal{O}(K \sqrt{T \Upsilon_T \log(T)})$
                            if $T$ and $\Upsilon_T$ are known
                      \item
                            $\cR(\cA,T) \leq \mathcal{O}(K \textcolor{orange}{\Upsilon_T} \sqrt{T \log(T)})$
                            if $T$ and \textcolor{orange}{$\Upsilon_T$ are unknown}
                  \end{itemize}
            \item
                  $\hookrightarrow$ our algorithm \textcolor{blue}{klUCB index + BGLR detector} is state-of-the-art!\\
                  \hfill{} \gray [Besson and Kaufmann, 19] \texttt{arXiv:1902.01575}
        \end{itemize}
    \end{colorblock}

\end{frameO}

\begin{frameO}[Results on a piece-wise stationary MAB problem]

    \textbf{Idea:} combine a good \textit{bandit algorithm} with a \emph{break-point detector}

    \begin{center}
        \includegraphics[width=0.90\textwidth]{regret_problem1.png}
    \end{center}

    \textcolor{blue}{klUCB + BGLR} achieves the best performance (among non-oracle)!
\end{frameO}

\begin{frameO}[More results ($1/3$)]

    \begin{center}
        \includegraphics[width=0.99\textwidth]{regret_problem2.png}
    \end{center}

    \textcolor{blue}{klUCB + BGLR} achieves the best performance (among non-oracle)!
\end{frameO}

\begin{frameO}[More results ($2/3$)]

    \begin{center}
        \includegraphics[width=0.99\textwidth]{regret_problem3.png}
    \end{center}

    \textcolor{blue}{klUCB + BGLR} achieves the best performance (among non-oracle)!
\end{frameO}

\begin{frameO}[More results ($3/3$)]

    \begin{center}
        \includegraphics[width=0.99\textwidth]{regret_problem4.png}
    \end{center}

    \textcolor{blue}{klUCB + BGLR} achieves the best performance (among non-oracle)!
\end{frameO}

\begin{frame}[shrink=50]
    \mytitle{centralesupelecdark}{centralesupelec}{State-of-the-art piece-wise stationary algorithms}

    \vspace*{2cm}

    \begin{tabular}{l | c | c | c | c}
        \textbf{Algorithm} & \textbf{Paper} & \textbf{Regret bound} $\cR(\cA, T)$ & \textbf{Complexity} & \textbf{Parameters} \\
        \hline
        Oracle-Restart UCB etc & \textcolor{gray}{[Auer et al. 2002]} & $\mathcal{O}(\frac{K}{\Delta_{\text{change}}^2} \Upsilon_T \log(T))$ & {\Large \textcolor{orange}{$\star$}} & unrealistic oracle \\
        Naive UCB etc & \textcolor{gray}{[Auer et al. 2002]} & $\mathcal{O}(T)$ in worst case & {\Large \textcolor{orange}{$\star$}} & none \\
        \hline
        Discounted UCB etc & \textcolor{gray}{[Garivier et al. 2009]} & $\mathcal{O}(C \sqrt{T \Upsilon_T} \log(T))$ & {\Large \textcolor{orange}{$\star$}} & $\gamma$ needs $\Upsilon_T$ \\
        Sliding-Window UCB etc & \textcolor{gray}{[Garivier et al. 2009]} & $\mathcal{O}(C \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star$}} & $w$ needs $\Upsilon_T$ \\
        \hline
        Exp3.S & \textcolor{gray}{[Auer et al. 2002]} & $\mathcal{O}(\frac{K}{\Delta_{\text{change}}^2} \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star$}} & $(\varepsilon_t)_t$ needs $\Upsilon_T$ \\
        \hline
        CUSUM-UCB & \textcolor{gray}{[Liu et al. 2018]} & $\mathcal{O}(C \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star\star$}} & needs $\Upsilon_T$ and $\delta < \Delta_{\text{change}}$ \\
        M-UCB & \textcolor{gray}{[Cao et al. 2019]} & $\mathcal{O}(C \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star\star$}} & needs $\Upsilon_T$ and $\delta < \Delta_{\text{change}}$ \\
        \textcolor{blue}{GLRT + kl-UCB} & \textcolor{gray}{[Besson et al. 2019]} & $\mathcal{O}(\frac{K}{\Delta_{\text{change}}^2} \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star\star$}} & needs $\Upsilon_T$ \\
        \hline
        AdSwitch & \textcolor{gray}{[Auer et al. 2019]} & $\mathcal{O}(C \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star\star\star\star$}} & none \\
        Ada-ILTCB+ & \textcolor{gray}{[Chen et al. 2019]} & $\mathcal{O}(C \sqrt{T \Upsilon_T \log(T)})$ & {\Large \textcolor{orange}{$\star\star\star\star\star$}} & none
    \end{tabular}

\end{frame}

% \begin{frameO}[More results ($4/4$)]

%     \begin{center}
%         \includegraphics[width=0.99\textwidth]{regret_problem5.png}
%     \end{center}

%     \textcolor{blue}{klUCB + BGLR} achieves the best performance (among non-oracle)!
% \end{frameO}

% \begin{frameO}[Other piece-wise stationary bandit models]
%     Our change-point detection GLR is useful for other bandit models

%     \only<1>{
%         \begin{center}
%             \includegraphics[width=0.90\textwidth]{GLRTest_for_PieceWise-Stationary_Combinatorial_Bandits.png}
%         \end{center}
%     }

%     \only<2>{
%         \begin{center}
%             \includegraphics[width=0.90\textwidth]{GLRTest_for_PieceWise-Stationary_Cascading_Bandits.png}
%         \end{center}
%     }
% \end{frameO}


% \section{Numerical simulations with SMPyBandits}

% \begin{frameTI}
%     \begin{center}
%         {\textcolor{white} {\Huge \textsc{Numerical simulations with SMPyBandits} }}
%     \end{center}
%     \vspace*{-4pt}
% \end{frameTI}

% \begin{frameO}[Numerical simulations with SMPyBandits]

%     FIXME

% \end{frameO}


\section{Summary}

\begin{frameTI}
    \begin{center}
        {\textcolor{white} {\Huge \textsc{Summary} }}
    \end{center}
    \vspace*{15pt}
    \begin{center}
        {\textcolor{white} {\huge \textsc{Thanks for your attention!} }}
    \end{center}
\end{frameTI}


\begin{frameO}[Conclusion: what we showed \hfill{} ($1/2$)]

    % \begin{colorblock}{What we showed}
        \large
        \begin{itemize}
            \setlength\itemsep{10pt}
            \item a simple model of IoT network, with decentralized learning embedded in the autonomous IoT devices,
            \item numerical simulations proving the quality of the proposed solution,
            \item a realistic implementation on radio hardware,
            % \item my Python library SMPyBandits.
            \item theoretical results in a simplified model: multi-player bandits,
            \item and an extension beyond the stationary hypothesis.
        \end{itemize}
    % \end{colorblock}

\end{frameO}


\begin{frameO}[Didn't have time to talk about\dots \hfill{} ($2/3$)]

    % \begin{darkblock}{Didn't have time to talk about\dots}
        \large
        \begin{itemize}
            \setlength\itemsep{10pt}
            \item my Python library SMPyBandits (Ch. 3),
            \item aggregation of bandit algorithms (Ch. 4),
            \item and details about our theoretical results and proofs (Ch. 6 \& 7),
            \item our work on the doubling trick (to make an algorithm $\cA$ anytime and keep its regret bounds).
        \end{itemize}
    % \end{darkblock}

\end{frameO}


\begin{frameO}[What could be done with another year(s) ? \hfill{} ($3/3$)]

    % \begin{lightblock}{What could be done with another year(s) ?}

        % \large
        \begin{itemize}
            \setlength\itemsep{2pt}
            \item Extend my Python library SMPyBandits to cover many other bandit models?
                (cascading, delay feedback, combinatorial, contextual etc)
            \item unify the multi-player and non-stationary bandit models\\
                $\hookrightarrow$ in progress: already one paper (arXiv 2019), we can probably do better with our tools!
            \item test our idea and our proposed algorithms in real-world environments\\
                $\hookrightarrow$ was started in summer $2019$ with an intern working with Christophe Moy
            \vspace*{10pt}
            \item the ``Graal'' goal:
                try to also let the devices learn \emph{when} to transmit (choose time), not only \emph{where/how} to transmit (choose frequency, etc)\dots\\
                \alert{Much harder!} search space is VERY LARGE!
            \vspace*{10pt}
            \item try more to study the ``selfish model'', for large $M$ nb of devices and small $p$ activation probabilities\dots?
        \end{itemize}
    % \end{lightblock}

\end{frameO}


\begin{frameO}[Where to know more: about bandits \hfill{} ($1/3$)]

    Check out the

    \vspace{1cm}

    \Huge
    \centering

    \href{https://tor-lattimore.com/downloads/book/book.pdf}{\textcolor{blue}{``The Bandit Book''}}

    \normalsize
    by Tor Lattimore and Csaba Szepesvári\\
    Cambridge University Press, 2019.

    \vspace{0.3cm}

    \normalsize $\hookrightarrow$ \href{https://tor-lattimore.com/downloads/book/book.pdf}{{\textcolor{blue}{\texttt{tor-lattimore.com/downloads/book/book.pdf}}}}

\end{frameO}


\begin{frameO}[Where to know more: about our work? \hfill{} ($2/3$)]

    Reach me (or Christophe Moy or Émilie Kaufmann) out by email, if you have questions

    \vspace{15pt}

    \large
    \centering

    \href{https://perso.crans.org/besson/}{\textcolor{darkgreen}{\texttt{Lilian.Besson @ CentraleSupelec.fr}}}

    $\hookrightarrow$ \href{https://perso.crans.org/besson/}{{\textcolor{darkgreen}{\texttt{perso.crans.org/besson/}}}}

    \vspace{15pt}

    \href{https://moychristophe.wordpress.com/}{\textcolor{orange}{\texttt{Christophe.Moy @ Univ-Rennes1.fr}}}

    $\hookrightarrow$ \href{https://moychristophe.wordpress.com/}{{\textcolor{orange}{\texttt{moychristophe.wordpress.com}}}}

    \vspace{15pt}

    \href{http://chercheurs.lille.inria.fr/ekaufman/}{\textcolor{orange}{\texttt{Emilie.Kaufmann @ Univ-Lille.fr}}}

    $\hookrightarrow$ \href{http://chercheurs.lille.inria.fr/ekaufman/}{{\textcolor{orange}{\texttt{chercheurs.lille.inria.fr/ekaufman}}}}

\end{frameO}


\begin{frameO}[Where to know more: in practice \hfill{} ($3/3$)]

    Experiment with bandits by yourself!

    \vspace{1cm}

    Interactive demo on this web-page\\
    $\hookrightarrow$ \href{https://perso.crans.org/besson/phd/MAB_interactive_demo/}{\textcolor{blue}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}}

    \vspace{1cm}

    Use our Python library for simulations of MAB problems \textbf{SMPyBandits} \\
    $\hookrightarrow$ \href{https://smpybandits.github.io/}{\textcolor{blue}{\texttt{SMPyBandits.GitHub.io}}}  \& \href{https://GitHub.com/SMPyBandits/}{\textcolor{blue}{\texttt{GitHub.com/SMPyBandits}}} \\
    \begin{itemize}
        \item Install with \texttt{$\$$ pip install SMPyBandits}
        \item Free and open-source (MIT license)
        \item Easy to set up your own bandit experiments, add new algorithms etc.
    \end{itemize}

\end{frameO}

\begin{frameO}[$\hookrightarrow$ \href{https://smpybandits.github.io/}{\textcolor{lightblue}{\texttt{SMPyBandits.GitHub.io}}}]

    \begin{center}
        \includegraphics[height=0.85\textheight]{overview_documentation_1}
    \end{center}

\end{frameO}


\begin{frameO}[Conclusion]

    \begin{center}
        \begin{huge}
            \textbf{Thanks for your attention!}
        \end{huge}
    \end{center}

    \vspace*{10pt}

    \begin{center}
        \begin{huge}
            Questions \& Discussion?
        \end{huge}
    \end{center}

    % \vspace*{10pt}
    % \pause

    % % FIXME pour la soutenance
    % \begin{center}
    %     \begin{huge}
    %         % FIXME pour la fausse soutenance à Lille
    %         \textcolor{darkgreen}{On se retrouve à 14h pour le pot (cafet Inria bât. A) !}
    %         % FIXME pour la vraie soutenance à Rennes
    %         \textcolor{darkgreen}{On se retrouve à 14h pour le pot (cafet 1er étage) !}
    %     \end{huge}
    % \end{center}

\end{frameO}


\begin{frameO}[Climatic crisis ?]

    \begin{center}
        \includegraphics[height=0.75\textheight]{../common/TalkAboutGlobalWarmingNow.png}

        \begin{tiny}
            \textcopyright{} Jeph Jacques, 2015,
            \href{https://QuestionableContent.net/view.php?comic=3074}{\textcolor{blue}{\texttt{QuestionableContent.net/view.php?comic=3074}}}
        \end{tiny}
    \end{center}

\end{frameO}

\begin{frameO}[Let's talk about actions against the climatic crisis !]

    \begin{center}
        \includegraphics[height=0.28\textheight]{../common/TalkAboutGlobalWarmingNow.png}
    \end{center}

    \begin{alertblock}{We are \emph{scientists}\ldots}
        Goals: \alert{inform ourselves, think, find, communicate}!
        \begin{itemize}
            \item \alert{Inform ourselves} of the \alert{causes} and \alert{consequences} of climatic crisis,
            \item \alert{Think} of all the problems, at political, local and individual scales,
            \item \alert{Find} simple solutions !\\
                  $\implies$ Aim at sobriety: transports, tourism, clothing, housing, technologies, food, computations, fighting smoking, etc.
            \item \alert{Communicate} our awareness, and our actions !
        \end{itemize}
    \end{alertblock}

\end{frameO}


\begin{frameTI}
    \begin{center}
        \color{white} \Huge \textsc{References and publications}

    \end{center}
    \vspace*{-4pt}
\end{frameTI}

\begin{frameO}[Main references]

    \begin{itemize}
        \item My PhD thesis (Lilian Besson)\\
              \href{https://github.com/Naereen/phd-thesis/}{\textcolor{blue}{``Multi-players Bandit Algorithms for Internet of Things Networks''}}\\
              $\hookrightarrow$ Online at \href{https://perso.crans.org/besson/phd/}{\textcolor{blue}{\texttt{perso.crans.org/besson/phd/}}}\\
              $\hookrightarrow$ Open-source at \href{https://github.com/Naereen/phd-thesis/}{\textcolor{blue}{\texttt{GitHub.com/Naereen/phd-thesis/}}}

              \vspace*{10pt}

        \item Our Python library for simulations of MAB problems, \textbf{SMPyBandits}\\
              $\hookrightarrow$ \href{https://smpybandits.github.io/}{\textcolor{blue}{\texttt{SMPyBandits.GitHub.io}}}

              \vspace*{10pt}

        \item \href{https://tor-lattimore.com/downloads/book/book.pdf}{\textcolor{blue}{``The Bandit Book''}},
              by Tor Lattimore and Csaba Szepesvari\\
              $\hookrightarrow$ \href{https://tor-lattimore.com/downloads/book/book.pdf}{{\textcolor{blue}{\texttt{tor-lattimore.com/downloads/book/book.pdf}}}}

              \vspace*{10pt}

        \item \href{https://arxiv.org/abs/1904.07272}{\textcolor{blue}{``Introduction to Multi-Armed Bandits''}},
              by Alex Slivkins\\
              $\hookrightarrow$ \href{https://arxiv.org/abs/1904.07272}{{\textcolor{blue}{\texttt{arXiv.org/abs/1904.07272}}}}

    \end{itemize}

\end{frameO}


\begin{frame}[c]
    \begin{changemargin}{-0.5cm}{-0.5cm}
        \begin{center}
            \vspace{-0.3in}
            \textbf{\huge List of publications}
            \vspace{1.5cm}
        \end{center}
    \end{changemargin}
\end{frame}


\begin{frame}[shrink=22]
    \mytitle{centralesupelecdark}{centralesupelec}{List of publications}

    \vspace*{2cm}

    \textcolor{orange}{International conferences with proceedings:}
    \begin{itemize}
        \item ``\emph{MAB Learning in IoT Networks}'', Bonnefoi, \textbf{Besson} et al, CROWNCOM \textcolor{blue}{2017}

        \item ``\emph{MALIN with GRC \dots}'', Bonnefoi, \textbf{Besson}, Moy, demo at ICT \textcolor{blue}{2018}
        \item ``\emph{Aggregation of MAB for OSA}'', \textbf{Besson}, Kaufmman, Moy, IEEE WCNC \textcolor{blue}{2018}
        \item ``\emph{Multi-Player Bandits Revisited}'', Bonnefoi \& Kaufmann, ALT \textcolor{blue}{2018}

        \item ``\emph{UCB \dots LPWAN w/ Retransmissions}'', Bonnefoi, \textbf{Besson} et al, IEEE WCNC \textcolor{blue}{2019}
        \item ``\emph{GNU Radio Implementation of MALIN \dots}'', \textbf{Besson} et al, IEEE WCNC \textcolor{blue}{2019}
        \item ``\emph{Decentralized Spectrum Learning \dots}'', Moy \& \textbf{Besson}, ISIoT \textcolor{blue}{2019}
        \item ``\emph{Analyse non asymptotique \dots}'', \textbf{Besson} \& Kaufmann, GRETSI \textcolor{blue}{2019}
    \end{itemize}

    \textcolor{orange}{Preprints:}
    \begin{itemize}
        \item ``\emph{Doubling-Trick \dots}'' \textbf{Besson} \& Kaufmann, \texttt{hal-01736357} \& \texttt{arXiv:1803.06971}, \textcolor{blue}{2018}
        % \item ``\emph{GLRT meets klUCB \dots}'', \textbf{Besson} \& Kaufmann, \texttt{hal-0\textcolor{blue}{2006471} }\texttt{arXiv:1902.01575}, \textcolor{blue}{2019}
        \item ``\emph{SMPyBandits \dots}'', \textbf{Besson}, \texttt{hal-01840022} \& \texttt{arXiv:1902.01575}, \textcolor{blue}{2018}
    \end{itemize}

    \textcolor{orange}{Submitted:}
    \begin{itemize}
        \item ``\emph{GLRT meets klUCB \dots}'', \textbf{Besson}, Kaufmann et al., submitted to AISTATS, \textcolor{blue}{October 2019}
        \item ``\emph{Decentralized Spectrum Learning \dots}'', Moy, \textbf{Besson} et al, submitted to Annals of Telecommunications, \textcolor{blue}{July 2019}
    \end{itemize}

\end{frame}


\begin{frameO}[International conferences with proceedings ($1/2)$]

    {\footnotesize

    \begin{itemize}
    \item
        \emph{Decentralized Spectrum Learning for IoT Wireless Networks Collision Mitigation},\\
        by Christophe Moy \& \textbf{Lilian Besson}.\\
        1st International ISIoT workshop,
        % \footnote{~See \href{https://sites.google.com/view/ISIoT2019}{\texttt{sites.google.com/view/ISIoT2019}}},
        at \emph{Conference on Distributed Computing in Sensor Systems},
        % \footnote{~IEEE DCOSS 2019, see \href{http://2019.dcoss.org}{\texttt{2019.dcoss.org}}},
        Santorini, Greece, May $2019$.\\
        % \href{https://HAL.Inria.fr/hal-02144465}{\texttt{HAL.Inria.fr/hal-02144465}}.
        \emph{See Chapter~5.}
        % \cite{MoyBesson2019}

        % \emph{$\hookrightarrow$ Note that we are already working on an extended version that will be submitted to a journal on Machine Learning for Wireless Communications, in July $2019$.}

    \item
        \emph{Upper-Confidence Bound for Channel Selection in LPWA Networks with Retransmissions},\\
        by Rémi Bonnefoi, \textbf{Lilian Besson}, Julio Manco-Vasquez \& Christophe Moy.\\
        1st International MOTIoN workshop,
        % \footnote{~MOTIoN 2019, see \href{https://sites.google.com/view/wcncworkshop-motion2019}{\texttt{sites.google.com/view/wcncworkshop-motion2019}}},
        at \emph{IEEE WCNC}, Marrakech, Morocco, April $2019$.\\
        % \href{https://HAL.Inria.fr/hal-02049824}{\texttt{HAL.Inria.fr/hal-02049824}}.
        \emph{See Section~5.4.}
        % \cite{Bonnefoi2019WCNC}

    \item
        \emph{GNU Radio Implementation of MALIN: ``Multi-Armed bandits Learning for Internet-of-things Networks''},\\
        by \textbf{Lilian Besson}, Rémi Bonnefoi \& Christophe Moy.\\
        \emph{Wireless Communication and Networks Conference},
        % \footnote{~IEEE WCNC 2019, see \href{http://wcnc2019.ieee-wcnc.org}{\texttt{wcnc2019.ieee-wcnc.org}}},
        Marrakech, Morocco, April $2019$,\\
        % \href{https://HAL.Inria.fr/hal-02006825}{\texttt{HAL.Inria.fr/hal-02006825}}.
        \emph{See Section~5.3.}
        % \cite{Besson2019WCNC}

    \end{itemize}

    For more details, see :
    \textcolor{blue}{\href{https://cv.archives-ouvertes.fr/lilian-besson/}{\texttt{CV.Archives-Ouvertes.fr/lilian-besson}}}.
    }
\end{frameO}


\begin{frameO}[International conferences with proceedings ($2/2)$]

    {\footnotesize

    \begin{itemize}
    \item
        \emph{Multi-Player Bandits Revisited},\\
        by \textbf{Lilian Besson} \& Émilie Kaufmann.\\
        \emph{Algorithmic Learning Theory},
        % \footnote{~ALT 2018, see \href{http://www.cs.cornell.edu/conferences/alt2018}{\texttt{www.cs.cornell.edu/conferences/alt2018}}},
        Lanzarote, Spain, April $2018$,\\
        % \href{https://HAL.Inria.fr/hal-01629733}{\texttt{HAL.Inria.fr/hal-01629733}}.
        \emph{See Chapter~6.}
        % \cite{Besson2018ALT}

    \item
        \emph{Aggregation of Multi-Armed Bandits learning algorithms for Opportunistic Spectrum Access},\\
        by \textbf{Lilian Besson}, Émilie Kaufmann \& Christophe Moy.\\
        \emph{Wireless Communication and Networks Conference},
        % \footnote{~IEEE WCNC 2018, see \href{http://wcnc2018.ieee-wcnc.org}{\texttt{wcnc2018.ieee-wcnc.org}}},
        Barcelona, Spain, April $2018$,\\
        % \href{https://HAL.Inria.fr/hal-01705292}{\texttt{HAL.Inria.fr/hal-01705292}}.
        \emph{See Chapter~4.}
        % \cite{Besson2018WCNC}

    \item
        \emph{Multi-Armed Bandit Learning in IoT Networks and non-stationary settings},\\
        by Rémi Bonnefoi, \textbf{L.Besson}, C.Moy, É.Kaufmann \& Jacques Palicot.\\
        \emph{Conference on Cognitive Radio Oriented Wireless Networks},
        % \footnote{~CROWNCOM 2017, see \href{http://crowncom.org/2017}{\texttt{crowncom.org/2017}}},
        Lisboa, Portugal, September $2017$,
        % \href{https://HAL.Inria.fr/hal-01575419}{\texttt{HAL.Inria.fr/hal-01575419}},
        \textbf{Best Paper Award}.\\
        \emph{See Section~5.2.}
        % \cite{Bonnefoi17}

    \end{itemize}
    }
\end{frameO}


\begin{frameO}[Demonstrations in international conferences]

{\footnotesize

    \begin{itemize}

    \item
        \emph{MALIN: ``Multi-Arm bandit Learning for Iot Networks'' with GRC: A TestBed Implementation and Demonstration that Learning Helps},\\
        by \textbf{Lilian Besson}, Rémi Bonnefoi, Christophe Moy.\\
        Demonstration presented in \emph{International Conference on Communication},
        % \footnote{~ICT 2018, see \href{http://ict-2018.org/demos}{\texttt{ict-2018.org/demos}}},
        Saint-Malo, France in June $2018$.\\
        See \href{https://YouTu.be/HospLNQhcMk}{\textcolor{blue}{\texttt{YouTu.be/HospLNQhcMk}}} for a $6$-minutes presentation video.\\
        \emph{See Section~5.3.}
        % \cite{Besson2018ICT}

    \end{itemize}
    }
\end{frameO}


\begin{frameO}[French language conferences with proceedings]

{\footnotesize

    \begin{itemize}
    \item
        \emph{Analyse non asymptotique d'un test séquentiel de détection de ruptures et application aux bandits non stationnaires} (in French),\\
        by \textbf{Lilian Besson} \& Émilie Kaufmann,\\
        GRETSI 2019,
        % \footnote{~GRETSI 2019, see \href{http://GRETSI.fr/colloque2019}{\texttt{GRETSI.fr/colloque2019}}},
        August $2019$,\\
        % \href{https://HAL.Inria.fr/hal-02006471}{\texttt{HAL.Inria.fr/hal-02006471}}.\\
        \emph{See Chapter~7.}
        % \cite{Besson2019Gretsi}

    \end{itemize}
    }
\end{frameO}


\begin{frameO}[Submitted works\dots]

{\footnotesize

    \begin{itemize}

    \item
        \emph{Decentralized Spectrum Learning for Radio Collision Mitigation in Ultra-Dense IoT Networks: LoRaWAN Case Study and Measurements},\\
        by Christophe Moy, \textbf{Lilian Besson}, Guillaume Delbarre \& Laurent Toutain,
        July $2019$.\\
        Submitted for a special volume of \href{https://annalsoftelecommunications.wp.imt.fr/}{the Annals of Telecommunications} journal, on ``Machine Learning for Intelligent Wireless Communications and Networking''.\\
        \emph{See Chapter~5.}
        % Preprint at \href{https://HAL.Inria.fr/hal-XXX}{\texttt{HAL.Inria.fr/hal-XXX}}.
        % \cite{MoyBesson2019Annales}

        % \emph{$\hookrightarrow$ Note that we are already working on an extended version that will be submitted to a journal on Machine Learning and Statistical Learning, in autumn $2019$.}

    \item
        \emph{The Generalized Likelihood Ratio Test meets klUCB: an Improved Algorithm for Piece-Wise Non-Stationary Bandits},\\
        by \textbf{Lilian Besson} \& Émilie Kaufmann,
        October $2019$.\\
        Submitted for \href{https://www.aistats.org/}{AISTATS 2020}.
        Preprint at \href{https://HAL.Inria.fr/hal-02006471}{\texttt{HAL.Inria.fr/hal-02006471}}.\\
        \emph{See Chapter~7.}
        % \cite{Besson2019GLRT}

    \end{itemize}
    }
\end{frameO}


\begin{frameO}[In progress works waiting for a new submission\dots]

{\footnotesize

    \begin{itemize}

    \item
        \emph{SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits (MAB) Algorithms in Python},\\
        by \textbf{Lilian Besson}\\
        Active development since October $2016$,
        \href{https://HAL.Inria.fr/hal-01840022}{\texttt{HAL.Inria.fr/hal-01840022}}.\\
        It currently consists in about $45000$ lines of code, hosted on \href{https://GitHub.com/SMPyBandits}{\texttt{GitHub.com/SMPyBandits}},
        and a complete documentation accessible on \href{https://SMPyBandits.rtfd.io}{\texttt{SMPyBandits.rtfd.io}} or \href{https://SMPyBandits.GitHub.io}{\texttt{SMPyBandits.GitHub.io}}.\\
        \emph{See Chapter~3.}
        % \cite{SMPyBanditsJMLR,SMPyBandits}

    \item
        \emph{What Doubling-Trick Can and Can't Do for Multi-Armed Bandits},\\
        by \textbf{Lilian Besson} \& Émilie Kaufmann,
        September $2018$.\\
        Preprint at \href{https://HAL.Inria.fr/hal-01736357}{\texttt{HAL.Inria.fr/hal-01736357}}.
        % \cite{Besson2018DoublingTricks}

    \end{itemize}
    }
\end{frameO}


% \begin{frameO}[References ($1/6$)]

%     TODO: update! remove a lot of references on bandits, and add on radio
%     TODO: add work of Wassim, Navik, Christophe, etc.
%     TODO: add a slide listing my publications?

%     {\footnotesize

%         \begin{itemize}
%             \item W.R. Thompson (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika.
%             \item H. Robbins (1952). Some aspects of the sequential design of experiments. \emph{Bulletin of the American Mathematical Society}.
%             \item Bradt, R., Johnson, S., and Karlin, S. (1956). On sequential designs for maximizing the sum of n observations. \emph{Annals of Mathematical Statistics}.
%             \item R. Bellman (1956). A problem in the sequential design of experiments. \emph{The indian journal of statistics}.
%             \item Gittins, J. (1979). Bandit processes and dynamic allocation indices. \emph{Journal of the Royal Statistical Society}.
%             \item Berry, D. and Fristedt, B. Bandit Problems (1985). Sequential allocation of experiments. \emph{Chapman and Hall}.
%             \item Lai, T. and Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. \emph{Advances in Applied Mathematics}.
%             \item Lai, T. (1987). Adaptive treatment allocation and the multi-armed bandit problem. \emph{Annals of Statistics}.
%         \end{itemize}
%     }

% \end{frameO}

% \begin{frameO}[References ($2/6$)]

%     {\footnotesize

%         \begin{itemize}
%             \item Agrawal, R. (1995). Sample mean based index policies with $\mathcal{O}(\log n)$ regret for the multi-armed bandit problem. \emph{Advances in Applied Probability}.
%             \item Katehakis, M. and Robbins, H. (1995). Sequential choice from several populations. \emph{Proceedings of the National Academy of Science}.
%             \item Burnetas, A. and Katehakis, M. (1996). Optimal adaptive policies for sequential allocation problems. \emph{Advances in Applied Mathematics}.
%             \item Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. \emph{Machine Learning}.
%             \item Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. (2002). The nonstochastic multiarmed bandit problem. \emph{SIAM Journal of Computing}.
%             \item Burnetas, A. and Katehakis, M. (2003). Asymptotic Bayes Analysis
%                   for the finite horizon one armed bandit problem. \emph{Probability in the Engineering and Informational Sciences}.
%             \item Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning and
%                   Games. \emph{Cambridge University Press}.
%             \item Audibert, J-Y., Munos, R. and Szepesvari, C. (2009). Exploration-exploitation trade-off using varianceestimates in multi-armed bandits. \emph{Theoretical Computer Science}.
%         \end{itemize}
%     }

% \end{frameO}

% \begin{frameO}[References ($3/6$)]

%     {\footnotesize

%         \begin{itemize}
%             \item Audibert, J.-Y. and Bubeck, S. (2010). Regret Bounds and Minimax Policies under Partial Monitoring. \emph{Journal of Machine Learning Research}.
%             \item Li, L., Chu, W., Langford, J. and Shapire, R. (2010). A Contextual-Bandit Approach to Personalized News Article Recommendation. \emph{WWW}.
%             \item Honda, J. and Takemura, A. (2010). An Asymptotically Optimal Bandit
%                   Algorithm for Bounded Support Models. \emph{COLT}.
%             \item Bubeck, S. (2010). Jeux de bandits et fondation du clustering. PhD thesis, Université de Lille 1.
%             \item A. Anandkumar, N. Michael, A. K. Tang, and S. Agrawal (2011). Distributed algorithms for learning and cognitive medium access with logarithmic regret. \emph{IEEE Journal on Selected Areas in Communications}
%             \item Garivier, A. and Cappé, O. (2011). The KL-UCB algorithm for bounded stochastic bandits and beyond. \emph{COLT}.
%             \item Maillard, O.-A., Munos, R., and Stoltz, G. (2011). A Finite-Time Analysis of Multi-armed Bandits Problems with Kullback-Leibler Divergences. \emph{COLT}.
%             \item Chapelle, O. and Li, L. (2011). An empirical evaluation of Thompson Sampling. \emph{NIPS}.
%         \end{itemize}
%     }

% \end{frameO}

% \begin{frameO}[References ($4/6$)]

%     {
%         \footnotesize

%         \begin{itemize}
%             \item {E. Kaufmann}, O. Cappé, A. Garivier (2012). On Bayesian Upper Confidence Bounds for Bandits Problems. \emph{AISTATS}.
%             \item Agrawal, S. and Goyal, N. (2012). Analysis of Thompson Sampling for the multi-armed bandit problem. \emph{COLT}.
%             \item E. Kaufmann, N. Korda, R. Munos (2012), {Thompson Sampling~: an Asymptotically Optimal Finite-Time Analysis}. \emph{Algorithmic Learning Theory}.
%             \item Bubeck, S. and Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. \emph{Fondations and Trends in Machine Learning}.
%             \item Agrawal, S. and Goyal, N. (2013). Further Optimal Regret Bounds for Thompson Sampling. \emph{AISTATS}.
%             \item O. Cappé, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz (2013). Kullback-Leibler upper confidence bounds for optimal sequential allocation. \emph{Annals of Statistics}.
%             \item Korda, N., Kaufmann, E., and Munos, R. (2013). Thompson Sampling for 1-dimensional Exponential family bandits. \emph{NIPS}.
%         \end{itemize}
%     }

% \end{frameO}

% \begin{frameO}[References ($5/6$)]

%     {\footnotesize

%         \begin{itemize}
%             \item Honda, J. and Takemura, A. (2014). Optimality of Thompson Sampling for Gaussian Bandits depends on priors. \emph{AISTATS}.
%             \item Baransi, Maillard, Mannor (2014). Sub-sampling for multi-armed bandits. \emph{ECML}.
%             \item Honda, J. and Takemura, A. (2015). Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards. \emph{JMLR}.
%             \item Kaufmann, E., Capp\'e O. and Garivier, A. (2016). On the complexity of best arm identification in multi-armed bandit problems. \emph{JMLR}
%             \item Lattimore, T. (2016). Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits. \emph{COLT}.
%             \item Garivier, A., Kaufmann, E. and Lattimore, T. (2016). On Explore-Then-Commit strategies. \emph{NIPS}.
%             \item E.Kaufmann (2017), On Bayesian index policies for sequential resource allocation. \emph{Annals of Statistics}.
%             \item Agrawal, S. and Goyal, N. (2017). Near-Optimal Regret Bounds for Thompson Sampling. \emph{Journal of ACM}.
%         \end{itemize}

%     }


% \end{frameO}

% \begin{frameO}[References ($6/6$)]

%     {\footnotesize
%         \begin{itemize}
%             \item Maillard, O-A (2017). Boundary Crossing for General Exponential Families. \emph{Algorithmic Learning Theory}.
%             \item Besson, L., Kaufmann E. (2018). Multi-Player Bandits Revisited. \emph{Algorithmic Learning Theory}.
%             \item Cowan, W., Honda, J. and Katehakis, M.N. (2018). Normal Bandits of Unknown Means and Variances. \emph{JMLR}.
%             \item Garivier,A. and Ménard, P. and Stoltz, G. (2018). Explore first, exploite next: the true shape of regret in bandit problems, \emph{Mathematics of Operations Research}
%             \item Garivier, A. and Hadiji, H. and Ménard, P. and Stoltz, G. (2018).
%                   KL-UCB-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints. \emph{arXiv: 1805.05071}.
%             \item Besson, L., Kaufmann E. (2019). The Generalized Likelihood Ratio Test meets klUCB: an Improved Algorithm for Piece-Wise Non-Stationary Bandits. \emph{Algorithmic Learning Theory}. \emph{arXiv: 1902.01575}.
%         \end{itemize}
%     }

% \end{frameO}


\end{document}
